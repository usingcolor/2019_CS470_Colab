{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6. Try and test your own network on Fashion-MNIST dataset.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cch98/2019_CS470_Colab/blob/master/6_Try_and_test_your_own_network_on_Fashion_MNIST_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRqPMAzzNipD",
        "colab_type": "text"
      },
      "source": [
        "Try and test your own network on Fashion-MNIST dataset\n",
        "====\n",
        "\n",
        "## Instruction\n",
        "- Go to step 5 to build blocks of your own network.\n",
        "- Train & test the network to see how good your model is.\n",
        "- You can refer to [this notebook](https://drive.google.com/open?id=1oO1cVmR_fyQFaIydRtYAs62StaQRJjEM) for stacking dropout and batch normalization layers\n",
        "- You can also try some hyper-parameter tunings (in step 3), such as trimming the learning rate, batch_size, etc.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RO1mgGV_uOIK",
        "colab_type": "text"
      },
      "source": [
        "## Step 1: Connect to your Google Drive\n",
        "\n",
        "It is required if you want to save checkpoints and load them later on"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLth6ZfXuSGT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "ccf626d1-124f-42d2-e9d2-9179f480228f"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/gdrive')\n",
        "gdrive_root = '/gdrive/My Drive'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYwUwGf8qW1U",
        "colab_type": "text"
      },
      "source": [
        "## Step 2: Import modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UtshANjqpy4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "bda54cd8-48b2-4bc2-fe9d-ed5b900ee1b4"
      },
      "source": [
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "# from torchvision.datasets import MNIST\n",
        "from torchvision.datasets import FashionMNIST as FMNIST\n",
        "\n",
        "# import TensorBoardColab\n",
        "!pip install -U tensorboardcolab\n",
        "from tensorboardcolab import TensorBoardColab\n",
        "\n",
        "torch.manual_seed(470)\n",
        "torch.cuda.manual_seed(470)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: tensorboardcolab in /usr/local/lib/python3.6/dist-packages (0.0.22)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iJ-Q6sbq8c3",
        "colab_type": "text"
      },
      "source": [
        "## Step 3: Configure the experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fA5jAy7Wq-E2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "3e3f65e8-92c4-4c0c-a032-263e2c85e410"
      },
      "source": [
        "# training & optimization hyper-parameters\n",
        "max_epoch = 200\n",
        "learning_rate = 0.0001\n",
        "batch_size = 20\n",
        "device = 'cuda'\n",
        "\n",
        "# model hyper-parameters\n",
        "input_dim = 784 # 28x28=784\n",
        "hidden_dim = 512\n",
        "output_dim = 10 \n",
        "\n",
        "# initialize tensorboard for visualization\n",
        "# Note : click the Tensorboard link to see the visualization of training/testing results\n",
        "tbc = TensorBoardColab()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wait for 8 seconds...\n",
            "TensorBoard link:\n",
            "https://17dc2c3f.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tZt60aMrQ1g",
        "colab_type": "text"
      },
      "source": [
        "## Step 4: Construct data pipeline\n",
        "\n",
        "**`torchvision.datasets.MNIST`** will automatically construct **`MNIST`** dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHbtV46LrXOF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_dir = os.path.join(gdrive_root, 'my_data')\n",
        "\n",
        "transform = transforms.ToTensor()\n",
        "\n",
        "# train_dataset = MNIST(data_dir, train=True, download=True, transform=transform)\n",
        "train_dataset = FMNIST(data_dir, train=True, download=True, transform=transform)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "\n",
        "# test_dataset = MNIST(data_dir, train=False, download=True, transform=transform)\n",
        "test_dataset = FMNIST(data_dir, train=False, download=True, transform=transform)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6G_dWd-6rwWb",
        "colab_type": "text"
      },
      "source": [
        "## Step 5: Construct a neural network builder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FX_wne0Vr1E5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyClassifier(nn.Module):\n",
        "  def __init__(self, input_dim=784, hidden_dim=512, output_dim=10):\n",
        "    super(MyClassifier, self).__init__()\n",
        "    self.layers = nn.Sequential(\n",
        "      # .. stack your layers here ...\n",
        "        nn.Conv2d(1, 20, 3, 1, 1),\n",
        "        nn.LeakyReLU(),\n",
        "        nn.Conv2d(20, 20, 3, 1, 1),\n",
        "        nn.LeakyReLU(),\n",
        "        nn.Conv2d(20, 10, 4, 2, 1),\n",
        "        nn.LeakyReLU(),\n",
        "        nn.Conv2d(10, 10, 4, 2, 1),\n",
        "        nn.LeakyReLU(),\n",
        "    )\n",
        "    self.layer2 = nn.Sequential(\n",
        "        nn.Linear(490, 100),\n",
        "        nn.LeakyReLU(),\n",
        "        nn.Linear(100, 10),\n",
        "        \n",
        "    )\n",
        "    \n",
        "  def forward(self, x):\n",
        "    batch_size = x.size(0)\n",
        "#     x = x.view(batch_size, -1)\n",
        "    x = self.layers(x)\n",
        "    x = x.view(batch_size, -1)\n",
        "    x = self.layer2(x)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpA3xhjMspvA",
        "colab_type": "text"
      },
      "source": [
        "## Step 6: Initialize the network and optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XP111gW0s8aH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_classifier = MyClassifier(input_dim, hidden_dim, output_dim)\n",
        "my_classifier = my_classifier.to(device)\n",
        "\n",
        "optimizer = optim.Adam(my_classifier.parameters(), lr=learning_rate)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lAQeXmjsILS",
        "colab_type": "text"
      },
      "source": [
        "## Step 7: Load pre-trained weights if exist"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFLNZxaBsHUl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f33c136b-277c-407e-f280-2ca06847c0a9"
      },
      "source": [
        "ckpt_dir = os.path.join(gdrive_root, 'checkpoints')\n",
        "if not os.path.exists(ckpt_dir):\n",
        "  os.makedirs(ckpt_dir)\n",
        "  \n",
        "best_acc = 0.\n",
        "ckpt_path = os.path.join(ckpt_dir, 'lastest.pt')\n",
        "if os.path.exists(ckpt_path):\n",
        "  ckpt = torch.load(ckpt_path)\n",
        "  try:\n",
        "    my_classifier.load_state_dict(ckpt['my_classifier'])\n",
        "    optimizer.load_state_dict(ckpt['optimizer'])\n",
        "    best_acc = ckpt['best_acc']\n",
        "  except RuntimeError as e:\n",
        "      print('wrong checkpoint')\n",
        "  else:    \n",
        "    print('checkpoint is loaded !')\n",
        "    print('current best accuracy : %.2f' % best_acc)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "checkpoint is loaded !\n",
            "current best accuracy : 0.91\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1t7n6yttNEc",
        "colab_type": "text"
      },
      "source": [
        "## Step 8: Train the network\n",
        "\n",
        "Note : It would be better to save checkpoints periodically, otherwise you'll lose everything you've trained if the session is recycled."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vczdKbytV38",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ff80f4ef-c8b4-4ea9-fc98-221d77a95ae8"
      },
      "source": [
        "\n",
        "it = 0\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "for epoch in range(max_epoch):\n",
        "  # train phase\n",
        "  my_classifier.train()\n",
        "  for inputs, labels in train_dataloader:\n",
        "    it += 1\n",
        "    \n",
        "    # load data to the GPU.\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "    \n",
        "    # feed data into the network and get outputs.\n",
        "    logits = my_classifier(inputs)\n",
        "    \n",
        "    # calculate loss\n",
        "    # Note: `F.cross_entropy` function receives logits, or pre-softmax outputs, rather than final probability scores.\n",
        "    loss = F.cross_entropy(logits, labels)\n",
        "    \n",
        "    # Note: You should flush out gradients computed at the previous step before computing gradients at the current step. \n",
        "    #       Otherwise, gradients will accumulate.\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    # backprogate loss.\n",
        "    loss.backward()\n",
        "    \n",
        "    # update the weights in the network.\n",
        "    optimizer.step()\n",
        "    \n",
        "    # calculate accuracy.\n",
        "    acc = (logits.argmax(dim=1) == labels).float().mean()\n",
        "    \n",
        "    if it % 200 == 0:\n",
        "      tbc.save_value('Loss', 'train_loss', it, loss.item())\n",
        "      print('[epoch:{}, iteration:{}] train loss : {:.4f} train accuracy : {:.4f}'.format(epoch, it, loss.item(), acc.item()))\n",
        "    \n",
        "  # save losses in a list so that we can visualize them later.\n",
        "  train_losses.append(loss.item())  \n",
        "    \n",
        "  # test phase\n",
        "  n = 0.\n",
        "  test_loss = 0.\n",
        "  test_acc = 0.\n",
        "  my_classifier.eval()\n",
        "  for test_inputs, test_labels in test_dataloader:\n",
        "    test_inputs = test_inputs.to(device)\n",
        "    test_labels = test_labels.to(device)\n",
        "    \n",
        "    logits = my_classifier(test_inputs)\n",
        "    test_loss += F.cross_entropy(logits, test_labels, reduction='sum')\n",
        "    test_acc += (logits.argmax(dim=1) == test_labels).float().sum()\n",
        "    n += inputs.size(0)\n",
        "    \n",
        "  test_loss /= n\n",
        "  test_acc /= n\n",
        "  test_losses.append(test_loss.item())\n",
        "  tbc.save_value('Loss', 'test_loss', it, test_loss.item())\n",
        "  print('[epoch:{}, iteration:{}] test_loss : {:.4f} test accuracy : {:.4f}'.format(epoch, it, test_loss.item(), test_acc.item())) \n",
        "  \n",
        "  tbc.flush_line('train_loss')\n",
        "  tbc.flush_line('test_loss')\n",
        "  \n",
        "  # save checkpoint whenever there is improvement in performance\n",
        "  if test_acc > best_acc:\n",
        "    best_acc = test_acc\n",
        "    # Note: optimizer also has states ! don't forget to save them as well.\n",
        "    ckpt = {'my_classifier':my_classifier.state_dict(),\n",
        "            'optimizer':optimizer.state_dict(),\n",
        "            'best_acc':best_acc}\n",
        "    torch.save(ckpt, ckpt_path)\n",
        "    print('checkpoint is saved !')\n",
        "    \n",
        "tbc.close()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[epoch:0, iteration:200] train loss : 1.5562 train accuracy : 0.5000\n",
            "[epoch:0, iteration:400] train loss : 0.6203 train accuracy : 0.7500\n",
            "[epoch:0, iteration:600] train loss : 1.0956 train accuracy : 0.7500\n",
            "[epoch:0, iteration:800] train loss : 0.6181 train accuracy : 0.7000\n",
            "[epoch:0, iteration:1000] train loss : 0.3144 train accuracy : 0.9500\n",
            "[epoch:0, iteration:1200] train loss : 0.5978 train accuracy : 0.7000\n",
            "[epoch:0, iteration:1400] train loss : 0.3838 train accuracy : 0.8000\n",
            "[epoch:0, iteration:1600] train loss : 0.6799 train accuracy : 0.8500\n",
            "[epoch:0, iteration:1800] train loss : 0.3789 train accuracy : 0.8000\n",
            "[epoch:0, iteration:2000] train loss : 0.5979 train accuracy : 0.7000\n",
            "[epoch:0, iteration:2200] train loss : 0.4427 train accuracy : 0.8500\n",
            "[epoch:0, iteration:2400] train loss : 0.3729 train accuracy : 0.9500\n",
            "[epoch:0, iteration:2600] train loss : 0.4241 train accuracy : 0.9000\n",
            "[epoch:0, iteration:2800] train loss : 0.4794 train accuracy : 0.8000\n",
            "[epoch:0, iteration:3000] train loss : 0.4623 train accuracy : 0.8500\n",
            "[epoch:0, iteration:3000] test_loss : 0.5191 test accuracy : 0.8107\n",
            "checkpoint is saved !\n",
            "[epoch:1, iteration:3200] train loss : 1.0592 train accuracy : 0.8000\n",
            "[epoch:1, iteration:3400] train loss : 0.3951 train accuracy : 0.8500\n",
            "[epoch:1, iteration:3600] train loss : 0.4681 train accuracy : 0.8500\n",
            "[epoch:1, iteration:3800] train loss : 0.2737 train accuracy : 0.9500\n",
            "[epoch:1, iteration:4000] train loss : 0.5572 train accuracy : 0.8500\n",
            "[epoch:1, iteration:4200] train loss : 0.5552 train accuracy : 0.8000\n",
            "[epoch:1, iteration:4400] train loss : 0.5759 train accuracy : 0.8000\n",
            "[epoch:1, iteration:4600] train loss : 0.3346 train accuracy : 0.9000\n",
            "[epoch:1, iteration:4800] train loss : 0.2637 train accuracy : 0.9500\n",
            "[epoch:1, iteration:5000] train loss : 0.5161 train accuracy : 0.8000\n",
            "[epoch:1, iteration:5200] train loss : 0.3298 train accuracy : 0.9500\n",
            "[epoch:1, iteration:5400] train loss : 0.6113 train accuracy : 0.9000\n",
            "[epoch:1, iteration:5600] train loss : 0.1337 train accuracy : 1.0000\n",
            "[epoch:1, iteration:5800] train loss : 0.7168 train accuracy : 0.6500\n",
            "[epoch:1, iteration:6000] train loss : 0.7270 train accuracy : 0.6500\n",
            "[epoch:1, iteration:6000] test_loss : 0.4466 test accuracy : 0.8336\n",
            "checkpoint is saved !\n",
            "[epoch:2, iteration:6200] train loss : 0.3471 train accuracy : 0.8500\n",
            "[epoch:2, iteration:6400] train loss : 0.0924 train accuracy : 1.0000\n",
            "[epoch:2, iteration:6600] train loss : 0.3899 train accuracy : 0.8500\n",
            "[epoch:2, iteration:6800] train loss : 0.3708 train accuracy : 0.8500\n",
            "[epoch:2, iteration:7000] train loss : 0.2222 train accuracy : 0.9500\n",
            "[epoch:2, iteration:7200] train loss : 0.2494 train accuracy : 0.9000\n",
            "[epoch:2, iteration:7400] train loss : 0.3811 train accuracy : 0.9000\n",
            "[epoch:2, iteration:7600] train loss : 0.5320 train accuracy : 0.7500\n",
            "[epoch:2, iteration:7800] train loss : 0.3566 train accuracy : 0.8500\n",
            "[epoch:2, iteration:8000] train loss : 0.2303 train accuracy : 0.9500\n",
            "[epoch:2, iteration:8200] train loss : 0.3191 train accuracy : 0.9000\n",
            "[epoch:2, iteration:8400] train loss : 0.5184 train accuracy : 0.7000\n",
            "[epoch:2, iteration:8600] train loss : 0.2606 train accuracy : 0.9000\n",
            "[epoch:2, iteration:8800] train loss : 0.5351 train accuracy : 0.7000\n",
            "[epoch:2, iteration:9000] train loss : 0.2396 train accuracy : 0.9500\n",
            "[epoch:2, iteration:9000] test_loss : 0.3916 test accuracy : 0.8586\n",
            "checkpoint is saved !\n",
            "[epoch:3, iteration:9200] train loss : 0.5755 train accuracy : 0.8000\n",
            "[epoch:3, iteration:9400] train loss : 0.5311 train accuracy : 0.8500\n",
            "[epoch:3, iteration:9600] train loss : 0.5481 train accuracy : 0.7000\n",
            "[epoch:3, iteration:9800] train loss : 0.3374 train accuracy : 0.9000\n",
            "[epoch:3, iteration:10000] train loss : 0.1801 train accuracy : 1.0000\n",
            "[epoch:3, iteration:10200] train loss : 0.2736 train accuracy : 0.9500\n",
            "[epoch:3, iteration:10400] train loss : 0.5640 train accuracy : 0.8000\n",
            "[epoch:3, iteration:10600] train loss : 0.2212 train accuracy : 0.9500\n",
            "[epoch:3, iteration:10800] train loss : 0.4049 train accuracy : 0.8000\n",
            "[epoch:3, iteration:11000] train loss : 0.7687 train accuracy : 0.7500\n",
            "[epoch:3, iteration:11200] train loss : 0.3283 train accuracy : 0.9000\n",
            "[epoch:3, iteration:11400] train loss : 0.0955 train accuracy : 1.0000\n",
            "[epoch:3, iteration:11600] train loss : 0.3253 train accuracy : 0.8500\n",
            "[epoch:3, iteration:11800] train loss : 0.2964 train accuracy : 0.8500\n",
            "[epoch:3, iteration:12000] train loss : 0.7008 train accuracy : 0.8000\n",
            "[epoch:3, iteration:12000] test_loss : 0.3670 test accuracy : 0.8665\n",
            "checkpoint is saved !\n",
            "[epoch:4, iteration:12200] train loss : 0.3398 train accuracy : 0.8500\n",
            "[epoch:4, iteration:12400] train loss : 0.3953 train accuracy : 0.8000\n",
            "[epoch:4, iteration:12600] train loss : 0.2785 train accuracy : 0.9500\n",
            "[epoch:4, iteration:12800] train loss : 0.6011 train accuracy : 0.8500\n",
            "[epoch:4, iteration:13000] train loss : 0.2266 train accuracy : 0.9000\n",
            "[epoch:4, iteration:13200] train loss : 0.3769 train accuracy : 0.8500\n",
            "[epoch:4, iteration:13400] train loss : 0.8360 train accuracy : 0.8500\n",
            "[epoch:4, iteration:13600] train loss : 0.6462 train accuracy : 0.9000\n",
            "[epoch:4, iteration:13800] train loss : 0.2225 train accuracy : 0.9000\n",
            "[epoch:4, iteration:14000] train loss : 0.1784 train accuracy : 0.9000\n",
            "[epoch:4, iteration:14200] train loss : 0.5978 train accuracy : 0.8000\n",
            "[epoch:4, iteration:14400] train loss : 0.5587 train accuracy : 0.8500\n",
            "[epoch:4, iteration:14600] train loss : 0.2675 train accuracy : 0.8500\n",
            "[epoch:4, iteration:14800] train loss : 0.2662 train accuracy : 0.8500\n",
            "[epoch:4, iteration:15000] train loss : 0.3653 train accuracy : 0.8500\n",
            "[epoch:4, iteration:15000] test_loss : 0.3563 test accuracy : 0.8681\n",
            "checkpoint is saved !\n",
            "[epoch:5, iteration:15200] train loss : 0.7416 train accuracy : 0.5500\n",
            "[epoch:5, iteration:15400] train loss : 0.2412 train accuracy : 0.9500\n",
            "[epoch:5, iteration:15600] train loss : 0.4169 train accuracy : 0.7500\n",
            "[epoch:5, iteration:15800] train loss : 0.4285 train accuracy : 0.8000\n",
            "[epoch:5, iteration:16000] train loss : 0.2005 train accuracy : 0.9500\n",
            "[epoch:5, iteration:16200] train loss : 0.2909 train accuracy : 0.9000\n",
            "[epoch:5, iteration:16400] train loss : 0.2016 train accuracy : 0.9500\n",
            "[epoch:5, iteration:16600] train loss : 0.4656 train accuracy : 0.7500\n",
            "[epoch:5, iteration:16800] train loss : 0.2717 train accuracy : 0.9000\n",
            "[epoch:5, iteration:17000] train loss : 0.4178 train accuracy : 0.8000\n",
            "[epoch:5, iteration:17200] train loss : 0.1649 train accuracy : 0.9500\n",
            "[epoch:5, iteration:17400] train loss : 0.2044 train accuracy : 0.9500\n",
            "[epoch:5, iteration:17600] train loss : 0.5464 train accuracy : 0.8500\n",
            "[epoch:5, iteration:17800] train loss : 0.0443 train accuracy : 1.0000\n",
            "[epoch:5, iteration:18000] train loss : 0.2311 train accuracy : 0.9000\n",
            "[epoch:5, iteration:18000] test_loss : 0.3458 test accuracy : 0.8719\n",
            "checkpoint is saved !\n",
            "[epoch:6, iteration:18200] train loss : 0.4280 train accuracy : 0.8000\n",
            "[epoch:6, iteration:18400] train loss : 0.1411 train accuracy : 1.0000\n",
            "[epoch:6, iteration:18600] train loss : 0.2153 train accuracy : 0.9500\n",
            "[epoch:6, iteration:18800] train loss : 0.4414 train accuracy : 0.9000\n",
            "[epoch:6, iteration:19000] train loss : 0.5780 train accuracy : 0.7000\n",
            "[epoch:6, iteration:19200] train loss : 0.4523 train accuracy : 0.8000\n",
            "[epoch:6, iteration:19400] train loss : 0.2081 train accuracy : 0.9500\n",
            "[epoch:6, iteration:19600] train loss : 0.2950 train accuracy : 0.8500\n",
            "[epoch:6, iteration:19800] train loss : 0.3453 train accuracy : 0.8500\n",
            "[epoch:6, iteration:20000] train loss : 0.2116 train accuracy : 0.9500\n",
            "[epoch:6, iteration:20200] train loss : 0.3201 train accuracy : 0.8500\n",
            "[epoch:6, iteration:20400] train loss : 0.7012 train accuracy : 0.8000\n",
            "[epoch:6, iteration:20600] train loss : 0.1781 train accuracy : 0.9500\n",
            "[epoch:6, iteration:20800] train loss : 0.4694 train accuracy : 0.7500\n",
            "[epoch:6, iteration:21000] train loss : 0.3229 train accuracy : 0.9000\n",
            "[epoch:6, iteration:21000] test_loss : 0.3229 test accuracy : 0.8800\n",
            "checkpoint is saved !\n",
            "[epoch:7, iteration:21200] train loss : 0.3537 train accuracy : 0.8500\n",
            "[epoch:7, iteration:21400] train loss : 0.4909 train accuracy : 0.9000\n",
            "[epoch:7, iteration:21600] train loss : 0.4942 train accuracy : 0.8000\n",
            "[epoch:7, iteration:21800] train loss : 0.2840 train accuracy : 0.8500\n",
            "[epoch:7, iteration:22000] train loss : 0.2042 train accuracy : 0.8500\n",
            "[epoch:7, iteration:22200] train loss : 0.3164 train accuracy : 0.9000\n",
            "[epoch:7, iteration:22400] train loss : 0.1183 train accuracy : 0.9500\n",
            "[epoch:7, iteration:22600] train loss : 0.4816 train accuracy : 0.6500\n",
            "[epoch:7, iteration:22800] train loss : 0.1221 train accuracy : 0.9500\n",
            "[epoch:7, iteration:23000] train loss : 0.4909 train accuracy : 0.8000\n",
            "[epoch:7, iteration:23200] train loss : 0.2845 train accuracy : 0.8500\n",
            "[epoch:7, iteration:23400] train loss : 0.3751 train accuracy : 0.8500\n",
            "[epoch:7, iteration:23600] train loss : 0.2957 train accuracy : 0.9000\n",
            "[epoch:7, iteration:23800] train loss : 0.1064 train accuracy : 1.0000\n",
            "[epoch:7, iteration:24000] train loss : 0.4355 train accuracy : 0.8500\n",
            "[epoch:7, iteration:24000] test_loss : 0.3175 test accuracy : 0.8830\n",
            "checkpoint is saved !\n",
            "[epoch:8, iteration:24200] train loss : 0.2286 train accuracy : 0.8500\n",
            "[epoch:8, iteration:24400] train loss : 0.2848 train accuracy : 0.9000\n",
            "[epoch:8, iteration:24600] train loss : 0.2234 train accuracy : 0.8500\n",
            "[epoch:8, iteration:24800] train loss : 0.0287 train accuracy : 1.0000\n",
            "[epoch:8, iteration:25000] train loss : 0.2273 train accuracy : 0.9000\n",
            "[epoch:8, iteration:25200] train loss : 0.6926 train accuracy : 0.7000\n",
            "[epoch:8, iteration:25400] train loss : 0.6295 train accuracy : 0.7500\n",
            "[epoch:8, iteration:25600] train loss : 0.2510 train accuracy : 0.9500\n",
            "[epoch:8, iteration:25800] train loss : 0.3562 train accuracy : 0.9000\n",
            "[epoch:8, iteration:26000] train loss : 0.4165 train accuracy : 0.8000\n",
            "[epoch:8, iteration:26200] train loss : 0.1376 train accuracy : 0.9500\n",
            "[epoch:8, iteration:26400] train loss : 0.2438 train accuracy : 0.9000\n",
            "[epoch:8, iteration:26600] train loss : 0.1309 train accuracy : 1.0000\n",
            "[epoch:8, iteration:26800] train loss : 0.3108 train accuracy : 0.9000\n",
            "[epoch:8, iteration:27000] train loss : 0.2087 train accuracy : 0.9500\n",
            "[epoch:8, iteration:27000] test_loss : 0.3029 test accuracy : 0.8876\n",
            "checkpoint is saved !\n",
            "[epoch:9, iteration:27200] train loss : 0.2472 train accuracy : 0.9000\n",
            "[epoch:9, iteration:27400] train loss : 0.4696 train accuracy : 0.9000\n",
            "[epoch:9, iteration:27600] train loss : 0.3308 train accuracy : 0.8000\n",
            "[epoch:9, iteration:27800] train loss : 0.1406 train accuracy : 0.9000\n",
            "[epoch:9, iteration:28000] train loss : 0.1572 train accuracy : 0.9500\n",
            "[epoch:9, iteration:28200] train loss : 0.3157 train accuracy : 0.9000\n",
            "[epoch:9, iteration:28400] train loss : 0.3561 train accuracy : 0.9000\n",
            "[epoch:9, iteration:28600] train loss : 0.2099 train accuracy : 0.9500\n",
            "[epoch:9, iteration:28800] train loss : 0.1525 train accuracy : 0.9500\n",
            "[epoch:9, iteration:29000] train loss : 0.2546 train accuracy : 0.9000\n",
            "[epoch:9, iteration:29200] train loss : 0.1740 train accuracy : 0.9000\n",
            "[epoch:9, iteration:29400] train loss : 0.3030 train accuracy : 0.9000\n",
            "[epoch:9, iteration:29600] train loss : 0.0450 train accuracy : 1.0000\n",
            "[epoch:9, iteration:29800] train loss : 0.1268 train accuracy : 0.9000\n",
            "[epoch:9, iteration:30000] train loss : 0.2775 train accuracy : 0.9000\n",
            "[epoch:9, iteration:30000] test_loss : 0.3008 test accuracy : 0.8883\n",
            "checkpoint is saved !\n",
            "[epoch:10, iteration:30200] train loss : 0.0880 train accuracy : 0.9500\n",
            "[epoch:10, iteration:30400] train loss : 0.0706 train accuracy : 0.9500\n",
            "[epoch:10, iteration:30600] train loss : 0.1679 train accuracy : 0.9500\n",
            "[epoch:10, iteration:30800] train loss : 0.3350 train accuracy : 0.8000\n",
            "[epoch:10, iteration:31000] train loss : 0.3370 train accuracy : 0.7500\n",
            "[epoch:10, iteration:31200] train loss : 0.4333 train accuracy : 0.9000\n",
            "[epoch:10, iteration:31400] train loss : 0.0670 train accuracy : 1.0000\n",
            "[epoch:10, iteration:31600] train loss : 0.1581 train accuracy : 0.9000\n",
            "[epoch:10, iteration:31800] train loss : 0.1624 train accuracy : 0.9500\n",
            "[epoch:10, iteration:32000] train loss : 0.0924 train accuracy : 0.9500\n",
            "[epoch:10, iteration:32200] train loss : 0.3435 train accuracy : 0.8500\n",
            "[epoch:10, iteration:32400] train loss : 0.1912 train accuracy : 0.9500\n",
            "[epoch:10, iteration:32600] train loss : 0.2636 train accuracy : 0.9500\n",
            "[epoch:10, iteration:32800] train loss : 0.1716 train accuracy : 0.9500\n",
            "[epoch:10, iteration:33000] train loss : 0.5771 train accuracy : 0.8500\n",
            "[epoch:10, iteration:33000] test_loss : 0.2974 test accuracy : 0.8896\n",
            "checkpoint is saved !\n",
            "[epoch:11, iteration:33200] train loss : 0.1902 train accuracy : 0.9500\n",
            "[epoch:11, iteration:33400] train loss : 0.0520 train accuracy : 1.0000\n",
            "[epoch:11, iteration:33600] train loss : 0.3793 train accuracy : 0.9500\n",
            "[epoch:11, iteration:33800] train loss : 0.4222 train accuracy : 0.8500\n",
            "[epoch:11, iteration:34000] train loss : 0.2097 train accuracy : 0.9000\n",
            "[epoch:11, iteration:34200] train loss : 0.2570 train accuracy : 0.9000\n",
            "[epoch:11, iteration:34400] train loss : 0.4519 train accuracy : 0.9500\n",
            "[epoch:11, iteration:34600] train loss : 0.1755 train accuracy : 0.9000\n",
            "[epoch:11, iteration:34800] train loss : 0.3797 train accuracy : 0.8500\n",
            "[epoch:11, iteration:35000] train loss : 0.0589 train accuracy : 1.0000\n",
            "[epoch:11, iteration:35200] train loss : 0.5088 train accuracy : 0.7500\n",
            "[epoch:11, iteration:35400] train loss : 0.4013 train accuracy : 0.9000\n",
            "[epoch:11, iteration:35600] train loss : 0.2296 train accuracy : 0.9000\n",
            "[epoch:11, iteration:35800] train loss : 0.2692 train accuracy : 0.8500\n",
            "[epoch:11, iteration:36000] train loss : 0.1345 train accuracy : 1.0000\n",
            "[epoch:11, iteration:36000] test_loss : 0.2828 test accuracy : 0.8959\n",
            "checkpoint is saved !\n",
            "[epoch:12, iteration:36200] train loss : 0.1401 train accuracy : 0.9500\n",
            "[epoch:12, iteration:36400] train loss : 0.2715 train accuracy : 0.9000\n",
            "[epoch:12, iteration:36600] train loss : 0.0477 train accuracy : 1.0000\n",
            "[epoch:12, iteration:36800] train loss : 0.1046 train accuracy : 0.9500\n",
            "[epoch:12, iteration:37000] train loss : 0.1802 train accuracy : 0.9000\n",
            "[epoch:12, iteration:37200] train loss : 0.0587 train accuracy : 1.0000\n",
            "[epoch:12, iteration:37400] train loss : 0.2404 train accuracy : 0.9000\n",
            "[epoch:12, iteration:37600] train loss : 0.1902 train accuracy : 0.9000\n",
            "[epoch:12, iteration:37800] train loss : 0.2319 train accuracy : 0.9500\n",
            "[epoch:12, iteration:38000] train loss : 0.1842 train accuracy : 0.9000\n",
            "[epoch:12, iteration:38200] train loss : 0.3353 train accuracy : 0.8500\n",
            "[epoch:12, iteration:38400] train loss : 0.3230 train accuracy : 0.8500\n",
            "[epoch:12, iteration:38600] train loss : 0.1001 train accuracy : 0.9500\n",
            "[epoch:12, iteration:38800] train loss : 0.0957 train accuracy : 1.0000\n",
            "[epoch:12, iteration:39000] train loss : 0.5490 train accuracy : 0.8000\n",
            "[epoch:12, iteration:39000] test_loss : 0.2893 test accuracy : 0.8909\n",
            "[epoch:13, iteration:39200] train loss : 0.0908 train accuracy : 1.0000\n",
            "[epoch:13, iteration:39400] train loss : 0.0533 train accuracy : 0.9500\n",
            "[epoch:13, iteration:39600] train loss : 0.2168 train accuracy : 0.9000\n",
            "[epoch:13, iteration:39800] train loss : 0.1918 train accuracy : 0.9500\n",
            "[epoch:13, iteration:40000] train loss : 0.0666 train accuracy : 1.0000\n",
            "[epoch:13, iteration:40200] train loss : 0.0714 train accuracy : 0.9500\n",
            "[epoch:13, iteration:40400] train loss : 0.2813 train accuracy : 0.8500\n",
            "[epoch:13, iteration:40600] train loss : 0.1191 train accuracy : 0.9500\n",
            "[epoch:13, iteration:40800] train loss : 0.1220 train accuracy : 1.0000\n",
            "[epoch:13, iteration:41000] train loss : 0.1767 train accuracy : 0.9000\n",
            "[epoch:13, iteration:41200] train loss : 0.0640 train accuracy : 1.0000\n",
            "[epoch:13, iteration:41400] train loss : 0.1793 train accuracy : 0.9000\n",
            "[epoch:13, iteration:41600] train loss : 0.1717 train accuracy : 0.9000\n",
            "[epoch:13, iteration:41800] train loss : 0.1532 train accuracy : 0.9500\n",
            "[epoch:13, iteration:42000] train loss : 0.4972 train accuracy : 0.8000\n",
            "[epoch:13, iteration:42000] test_loss : 0.2816 test accuracy : 0.8980\n",
            "checkpoint is saved !\n",
            "[epoch:14, iteration:42200] train loss : 0.3806 train accuracy : 0.8500\n",
            "[epoch:14, iteration:42400] train loss : 0.2632 train accuracy : 0.8500\n",
            "[epoch:14, iteration:42600] train loss : 0.2251 train accuracy : 0.9000\n",
            "[epoch:14, iteration:42800] train loss : 0.6942 train accuracy : 0.8000\n",
            "[epoch:14, iteration:43000] train loss : 0.3742 train accuracy : 0.9000\n",
            "[epoch:14, iteration:43200] train loss : 0.2915 train accuracy : 0.8500\n",
            "[epoch:14, iteration:43400] train loss : 0.1602 train accuracy : 1.0000\n",
            "[epoch:14, iteration:43600] train loss : 0.1547 train accuracy : 0.9500\n",
            "[epoch:14, iteration:43800] train loss : 0.2563 train accuracy : 0.9000\n",
            "[epoch:14, iteration:44000] train loss : 0.2750 train accuracy : 0.8500\n",
            "[epoch:14, iteration:44200] train loss : 0.3652 train accuracy : 0.8000\n",
            "[epoch:14, iteration:44400] train loss : 0.1218 train accuracy : 0.9500\n",
            "[epoch:14, iteration:44600] train loss : 0.2577 train accuracy : 0.9500\n",
            "[epoch:14, iteration:44800] train loss : 0.0612 train accuracy : 1.0000\n",
            "[epoch:14, iteration:45000] train loss : 0.1624 train accuracy : 0.9000\n",
            "[epoch:14, iteration:45000] test_loss : 0.2755 test accuracy : 0.8988\n",
            "checkpoint is saved !\n",
            "[epoch:15, iteration:45200] train loss : 0.2405 train accuracy : 0.9000\n",
            "[epoch:15, iteration:45400] train loss : 0.3642 train accuracy : 0.9500\n",
            "[epoch:15, iteration:45600] train loss : 0.0785 train accuracy : 0.9500\n",
            "[epoch:15, iteration:45800] train loss : 0.0739 train accuracy : 0.9500\n",
            "[epoch:15, iteration:46000] train loss : 0.3246 train accuracy : 0.8000\n",
            "[epoch:15, iteration:46200] train loss : 0.1651 train accuracy : 0.9500\n",
            "[epoch:15, iteration:46400] train loss : 0.3111 train accuracy : 0.8500\n",
            "[epoch:15, iteration:46600] train loss : 0.2708 train accuracy : 0.9000\n",
            "[epoch:15, iteration:46800] train loss : 0.1822 train accuracy : 0.9000\n",
            "[epoch:15, iteration:47000] train loss : 0.1020 train accuracy : 0.9500\n",
            "[epoch:15, iteration:47200] train loss : 0.2736 train accuracy : 0.8500\n",
            "[epoch:15, iteration:47400] train loss : 0.1690 train accuracy : 0.9500\n",
            "[epoch:15, iteration:47600] train loss : 0.2324 train accuracy : 0.9500\n",
            "[epoch:15, iteration:47800] train loss : 0.1613 train accuracy : 0.9500\n",
            "[epoch:15, iteration:48000] train loss : 0.1883 train accuracy : 0.9000\n",
            "[epoch:15, iteration:48000] test_loss : 0.2736 test accuracy : 0.9014\n",
            "checkpoint is saved !\n",
            "[epoch:16, iteration:48200] train loss : 0.0589 train accuracy : 1.0000\n",
            "[epoch:16, iteration:48400] train loss : 0.1836 train accuracy : 0.9000\n",
            "[epoch:16, iteration:48600] train loss : 0.1535 train accuracy : 0.9000\n",
            "[epoch:16, iteration:48800] train loss : 0.1103 train accuracy : 1.0000\n",
            "[epoch:16, iteration:49000] train loss : 0.1795 train accuracy : 0.9000\n",
            "[epoch:16, iteration:49200] train loss : 0.2702 train accuracy : 0.9000\n",
            "[epoch:16, iteration:49400] train loss : 0.2163 train accuracy : 0.9000\n",
            "[epoch:16, iteration:49600] train loss : 0.2661 train accuracy : 0.9000\n",
            "[epoch:16, iteration:49800] train loss : 0.1637 train accuracy : 0.9000\n",
            "[epoch:16, iteration:50000] train loss : 0.1409 train accuracy : 0.9000\n",
            "[epoch:16, iteration:50200] train loss : 0.3035 train accuracy : 0.8500\n",
            "[epoch:16, iteration:50400] train loss : 0.1468 train accuracy : 0.9500\n",
            "[epoch:16, iteration:50600] train loss : 0.1116 train accuracy : 0.9500\n",
            "[epoch:16, iteration:50800] train loss : 0.1067 train accuracy : 0.9500\n",
            "[epoch:16, iteration:51000] train loss : 0.0731 train accuracy : 1.0000\n",
            "[epoch:16, iteration:51000] test_loss : 0.2722 test accuracy : 0.9044\n",
            "checkpoint is saved !\n",
            "[epoch:17, iteration:51200] train loss : 0.2068 train accuracy : 0.9500\n",
            "[epoch:17, iteration:51400] train loss : 0.3973 train accuracy : 0.9000\n",
            "[epoch:17, iteration:51600] train loss : 0.0369 train accuracy : 1.0000\n",
            "[epoch:17, iteration:51800] train loss : 0.1205 train accuracy : 0.9500\n",
            "[epoch:17, iteration:52000] train loss : 0.0424 train accuracy : 1.0000\n",
            "[epoch:17, iteration:52200] train loss : 0.0569 train accuracy : 1.0000\n",
            "[epoch:17, iteration:52400] train loss : 0.0394 train accuracy : 1.0000\n",
            "[epoch:17, iteration:52600] train loss : 0.1780 train accuracy : 0.9500\n",
            "[epoch:17, iteration:52800] train loss : 0.1211 train accuracy : 0.9500\n",
            "[epoch:17, iteration:53000] train loss : 0.2752 train accuracy : 0.8500\n",
            "[epoch:17, iteration:53200] train loss : 0.2260 train accuracy : 0.9500\n",
            "[epoch:17, iteration:53400] train loss : 0.2439 train accuracy : 0.9000\n",
            "[epoch:17, iteration:53600] train loss : 0.4576 train accuracy : 0.7500\n",
            "[epoch:17, iteration:53800] train loss : 0.1551 train accuracy : 0.9500\n",
            "[epoch:17, iteration:54000] train loss : 0.3985 train accuracy : 0.8500\n",
            "[epoch:17, iteration:54000] test_loss : 0.2685 test accuracy : 0.9027\n",
            "[epoch:18, iteration:54200] train loss : 0.2091 train accuracy : 0.9500\n",
            "[epoch:18, iteration:54400] train loss : 0.1497 train accuracy : 0.9500\n",
            "[epoch:18, iteration:54600] train loss : 0.6438 train accuracy : 0.9000\n",
            "[epoch:18, iteration:54800] train loss : 0.1219 train accuracy : 0.9500\n",
            "[epoch:18, iteration:55000] train loss : 0.1298 train accuracy : 0.9000\n",
            "[epoch:18, iteration:55200] train loss : 0.2914 train accuracy : 0.8500\n",
            "[epoch:18, iteration:55400] train loss : 0.2676 train accuracy : 0.8500\n",
            "[epoch:18, iteration:55600] train loss : 0.1789 train accuracy : 0.9000\n",
            "[epoch:18, iteration:55800] train loss : 0.0837 train accuracy : 1.0000\n",
            "[epoch:18, iteration:56000] train loss : 0.2368 train accuracy : 0.9000\n",
            "[epoch:18, iteration:56200] train loss : 0.1377 train accuracy : 0.9500\n",
            "[epoch:18, iteration:56400] train loss : 0.0295 train accuracy : 1.0000\n",
            "[epoch:18, iteration:56600] train loss : 0.3042 train accuracy : 0.9000\n",
            "[epoch:18, iteration:56800] train loss : 0.3716 train accuracy : 0.8500\n",
            "[epoch:18, iteration:57000] train loss : 0.1128 train accuracy : 0.9500\n",
            "[epoch:18, iteration:57000] test_loss : 0.2754 test accuracy : 0.8999\n",
            "[epoch:19, iteration:57200] train loss : 0.0863 train accuracy : 1.0000\n",
            "[epoch:19, iteration:57400] train loss : 0.0316 train accuracy : 1.0000\n",
            "[epoch:19, iteration:57600] train loss : 0.2753 train accuracy : 0.9500\n",
            "[epoch:19, iteration:57800] train loss : 0.3863 train accuracy : 0.9000\n",
            "[epoch:19, iteration:58000] train loss : 0.1930 train accuracy : 0.9000\n",
            "[epoch:19, iteration:58200] train loss : 0.1687 train accuracy : 1.0000\n",
            "[epoch:19, iteration:58400] train loss : 0.2852 train accuracy : 0.8500\n",
            "[epoch:19, iteration:58600] train loss : 0.2983 train accuracy : 0.9000\n",
            "[epoch:19, iteration:58800] train loss : 0.1439 train accuracy : 0.9500\n",
            "[epoch:19, iteration:59000] train loss : 0.1174 train accuracy : 0.9500\n",
            "[epoch:19, iteration:59200] train loss : 0.0929 train accuracy : 1.0000\n",
            "[epoch:19, iteration:59400] train loss : 0.3210 train accuracy : 0.9000\n",
            "[epoch:19, iteration:59600] train loss : 0.0438 train accuracy : 1.0000\n",
            "[epoch:19, iteration:59800] train loss : 0.2552 train accuracy : 0.9000\n",
            "[epoch:19, iteration:60000] train loss : 0.0802 train accuracy : 0.9500\n",
            "[epoch:19, iteration:60000] test_loss : 0.2651 test accuracy : 0.9081\n",
            "checkpoint is saved !\n",
            "[epoch:20, iteration:60200] train loss : 0.1133 train accuracy : 1.0000\n",
            "[epoch:20, iteration:60400] train loss : 0.1248 train accuracy : 0.9500\n",
            "[epoch:20, iteration:60600] train loss : 0.0426 train accuracy : 1.0000\n",
            "[epoch:20, iteration:60800] train loss : 0.0648 train accuracy : 1.0000\n",
            "[epoch:20, iteration:61000] train loss : 0.1561 train accuracy : 0.9500\n",
            "[epoch:20, iteration:61200] train loss : 0.1001 train accuracy : 1.0000\n",
            "[epoch:20, iteration:61400] train loss : 0.1314 train accuracy : 0.9500\n",
            "[epoch:20, iteration:61600] train loss : 0.3538 train accuracy : 0.9500\n",
            "[epoch:20, iteration:61800] train loss : 0.2893 train accuracy : 0.9000\n",
            "[epoch:20, iteration:62000] train loss : 0.1194 train accuracy : 0.9500\n",
            "[epoch:20, iteration:62200] train loss : 0.0649 train accuracy : 1.0000\n",
            "[epoch:20, iteration:62400] train loss : 0.2884 train accuracy : 0.8500\n",
            "[epoch:20, iteration:62600] train loss : 0.2411 train accuracy : 0.9000\n",
            "[epoch:20, iteration:62800] train loss : 0.2126 train accuracy : 0.8000\n",
            "[epoch:20, iteration:63000] train loss : 0.1370 train accuracy : 0.9500\n",
            "[epoch:20, iteration:63000] test_loss : 0.2742 test accuracy : 0.9026\n",
            "[epoch:21, iteration:63200] train loss : 0.1931 train accuracy : 0.9500\n",
            "[epoch:21, iteration:63400] train loss : 0.1160 train accuracy : 0.9500\n",
            "[epoch:21, iteration:63600] train loss : 0.1314 train accuracy : 0.9500\n",
            "[epoch:21, iteration:63800] train loss : 0.2280 train accuracy : 0.8500\n",
            "[epoch:21, iteration:64000] train loss : 0.3330 train accuracy : 0.8000\n",
            "[epoch:21, iteration:64200] train loss : 0.1121 train accuracy : 1.0000\n",
            "[epoch:21, iteration:64400] train loss : 0.0732 train accuracy : 1.0000\n",
            "[epoch:21, iteration:64600] train loss : 0.1136 train accuracy : 0.9500\n",
            "[epoch:21, iteration:64800] train loss : 0.1626 train accuracy : 0.9500\n",
            "[epoch:21, iteration:65000] train loss : 0.0703 train accuracy : 0.9500\n",
            "[epoch:21, iteration:65200] train loss : 0.5230 train accuracy : 0.8000\n",
            "[epoch:21, iteration:65400] train loss : 0.2290 train accuracy : 0.8500\n",
            "[epoch:21, iteration:65600] train loss : 0.1328 train accuracy : 1.0000\n",
            "[epoch:21, iteration:65800] train loss : 0.0568 train accuracy : 1.0000\n",
            "[epoch:21, iteration:66000] train loss : 0.2125 train accuracy : 0.9500\n",
            "[epoch:21, iteration:66000] test_loss : 0.2626 test accuracy : 0.9103\n",
            "checkpoint is saved !\n",
            "[epoch:22, iteration:66200] train loss : 0.1236 train accuracy : 0.9500\n",
            "[epoch:22, iteration:66400] train loss : 0.1123 train accuracy : 0.9500\n",
            "[epoch:22, iteration:66600] train loss : 0.0911 train accuracy : 1.0000\n",
            "[epoch:22, iteration:66800] train loss : 0.2706 train accuracy : 0.8500\n",
            "[epoch:22, iteration:67000] train loss : 0.0094 train accuracy : 1.0000\n",
            "[epoch:22, iteration:67200] train loss : 0.1830 train accuracy : 0.9500\n",
            "[epoch:22, iteration:67400] train loss : 0.1979 train accuracy : 0.9500\n",
            "[epoch:22, iteration:67600] train loss : 0.1499 train accuracy : 0.9500\n",
            "[epoch:22, iteration:67800] train loss : 0.2385 train accuracy : 0.9500\n",
            "[epoch:22, iteration:68000] train loss : 0.1322 train accuracy : 0.9500\n",
            "[epoch:22, iteration:68200] train loss : 0.0146 train accuracy : 1.0000\n",
            "[epoch:22, iteration:68400] train loss : 0.3268 train accuracy : 0.9000\n",
            "[epoch:22, iteration:68600] train loss : 0.0333 train accuracy : 1.0000\n",
            "[epoch:22, iteration:68800] train loss : 0.0086 train accuracy : 1.0000\n",
            "[epoch:22, iteration:69000] train loss : 0.1071 train accuracy : 0.9000\n",
            "[epoch:22, iteration:69000] test_loss : 0.2648 test accuracy : 0.9070\n",
            "[epoch:23, iteration:69200] train loss : 0.3518 train accuracy : 0.8500\n",
            "[epoch:23, iteration:69400] train loss : 0.0311 train accuracy : 1.0000\n",
            "[epoch:23, iteration:69600] train loss : 0.1024 train accuracy : 0.9500\n",
            "[epoch:23, iteration:69800] train loss : 0.0322 train accuracy : 1.0000\n",
            "[epoch:23, iteration:70000] train loss : 0.1132 train accuracy : 1.0000\n",
            "[epoch:23, iteration:70200] train loss : 0.2829 train accuracy : 0.8500\n",
            "[epoch:23, iteration:70400] train loss : 0.1929 train accuracy : 0.9000\n",
            "[epoch:23, iteration:70600] train loss : 0.0420 train accuracy : 1.0000\n",
            "[epoch:23, iteration:70800] train loss : 0.1600 train accuracy : 0.9500\n",
            "[epoch:23, iteration:71000] train loss : 0.2797 train accuracy : 0.9500\n",
            "[epoch:23, iteration:71200] train loss : 0.0063 train accuracy : 1.0000\n",
            "[epoch:23, iteration:71400] train loss : 0.0285 train accuracy : 1.0000\n",
            "[epoch:23, iteration:71600] train loss : 0.1188 train accuracy : 0.9500\n",
            "[epoch:23, iteration:71800] train loss : 0.4042 train accuracy : 0.8500\n",
            "[epoch:23, iteration:72000] train loss : 0.1659 train accuracy : 0.9000\n",
            "[epoch:23, iteration:72000] test_loss : 0.2667 test accuracy : 0.9088\n",
            "[epoch:24, iteration:72200] train loss : 0.0481 train accuracy : 1.0000\n",
            "[epoch:24, iteration:72400] train loss : 0.0429 train accuracy : 1.0000\n",
            "[epoch:24, iteration:72600] train loss : 0.0935 train accuracy : 1.0000\n",
            "[epoch:24, iteration:72800] train loss : 0.1714 train accuracy : 0.9000\n",
            "[epoch:24, iteration:73000] train loss : 0.1187 train accuracy : 0.9500\n",
            "[epoch:24, iteration:73200] train loss : 0.0826 train accuracy : 1.0000\n",
            "[epoch:24, iteration:73400] train loss : 0.5617 train accuracy : 0.8500\n",
            "[epoch:24, iteration:73600] train loss : 0.0835 train accuracy : 0.9500\n",
            "[epoch:24, iteration:73800] train loss : 0.1653 train accuracy : 0.9000\n",
            "[epoch:24, iteration:74000] train loss : 0.1472 train accuracy : 0.9500\n",
            "[epoch:24, iteration:74200] train loss : 0.1233 train accuracy : 0.9500\n",
            "[epoch:24, iteration:74400] train loss : 0.1346 train accuracy : 0.9500\n",
            "[epoch:24, iteration:74600] train loss : 0.0461 train accuracy : 1.0000\n",
            "[epoch:24, iteration:74800] train loss : 0.3710 train accuracy : 0.9000\n",
            "[epoch:24, iteration:75000] train loss : 0.0810 train accuracy : 1.0000\n",
            "[epoch:24, iteration:75000] test_loss : 0.2662 test accuracy : 0.9080\n",
            "[epoch:25, iteration:75200] train loss : 0.3648 train accuracy : 0.9500\n",
            "[epoch:25, iteration:75400] train loss : 0.0495 train accuracy : 0.9500\n",
            "[epoch:25, iteration:75600] train loss : 0.4309 train accuracy : 0.8500\n",
            "[epoch:25, iteration:75800] train loss : 0.2213 train accuracy : 0.9000\n",
            "[epoch:25, iteration:76000] train loss : 0.2727 train accuracy : 0.8500\n",
            "[epoch:25, iteration:76200] train loss : 0.0298 train accuracy : 1.0000\n",
            "[epoch:25, iteration:76400] train loss : 0.0625 train accuracy : 1.0000\n",
            "[epoch:25, iteration:76600] train loss : 0.1525 train accuracy : 0.9000\n",
            "[epoch:25, iteration:76800] train loss : 0.0797 train accuracy : 1.0000\n",
            "[epoch:25, iteration:77000] train loss : 0.0455 train accuracy : 1.0000\n",
            "[epoch:25, iteration:77200] train loss : 0.4385 train accuracy : 0.9000\n",
            "[epoch:25, iteration:77400] train loss : 0.1587 train accuracy : 0.9000\n",
            "[epoch:25, iteration:77600] train loss : 0.1904 train accuracy : 0.9000\n",
            "[epoch:25, iteration:77800] train loss : 0.0483 train accuracy : 1.0000\n",
            "[epoch:25, iteration:78000] train loss : 0.0376 train accuracy : 1.0000\n",
            "[epoch:25, iteration:78000] test_loss : 0.2632 test accuracy : 0.9102\n",
            "[epoch:26, iteration:78200] train loss : 0.2031 train accuracy : 0.9000\n",
            "[epoch:26, iteration:78400] train loss : 0.0764 train accuracy : 1.0000\n",
            "[epoch:26, iteration:78600] train loss : 0.2008 train accuracy : 0.9000\n",
            "[epoch:26, iteration:78800] train loss : 0.1121 train accuracy : 0.9500\n",
            "[epoch:26, iteration:79000] train loss : 0.2940 train accuracy : 0.9000\n",
            "[epoch:26, iteration:79200] train loss : 0.2009 train accuracy : 0.9500\n",
            "[epoch:26, iteration:79400] train loss : 0.0693 train accuracy : 0.9500\n",
            "[epoch:26, iteration:79600] train loss : 0.1480 train accuracy : 0.9000\n",
            "[epoch:26, iteration:79800] train loss : 0.0310 train accuracy : 1.0000\n",
            "[epoch:26, iteration:80000] train loss : 0.0816 train accuracy : 0.9500\n",
            "[epoch:26, iteration:80200] train loss : 0.2030 train accuracy : 0.9500\n",
            "[epoch:26, iteration:80400] train loss : 0.1716 train accuracy : 0.9500\n",
            "[epoch:26, iteration:80600] train loss : 0.1259 train accuracy : 0.9500\n",
            "[epoch:26, iteration:80800] train loss : 0.0959 train accuracy : 0.9000\n",
            "[epoch:26, iteration:81000] train loss : 0.2753 train accuracy : 0.8500\n",
            "[epoch:26, iteration:81000] test_loss : 0.2708 test accuracy : 0.9118\n",
            "checkpoint is saved !\n",
            "[epoch:27, iteration:81200] train loss : 0.1901 train accuracy : 0.9500\n",
            "[epoch:27, iteration:81400] train loss : 0.0784 train accuracy : 0.9500\n",
            "[epoch:27, iteration:81600] train loss : 0.1770 train accuracy : 0.9500\n",
            "[epoch:27, iteration:81800] train loss : 0.4368 train accuracy : 0.9000\n",
            "[epoch:27, iteration:82000] train loss : 0.1295 train accuracy : 0.9500\n",
            "[epoch:27, iteration:82200] train loss : 0.2089 train accuracy : 0.9000\n",
            "[epoch:27, iteration:82400] train loss : 0.0412 train accuracy : 1.0000\n",
            "[epoch:27, iteration:82600] train loss : 0.3762 train accuracy : 0.9500\n",
            "[epoch:27, iteration:82800] train loss : 0.0461 train accuracy : 1.0000\n",
            "[epoch:27, iteration:83000] train loss : 0.2397 train accuracy : 0.9500\n",
            "[epoch:27, iteration:83200] train loss : 0.1540 train accuracy : 0.9000\n",
            "[epoch:27, iteration:83400] train loss : 0.1901 train accuracy : 0.9500\n",
            "[epoch:27, iteration:83600] train loss : 0.1215 train accuracy : 1.0000\n",
            "[epoch:27, iteration:83800] train loss : 0.0364 train accuracy : 1.0000\n",
            "[epoch:27, iteration:84000] train loss : 0.2094 train accuracy : 0.9000\n",
            "[epoch:27, iteration:84000] test_loss : 0.2702 test accuracy : 0.9076\n",
            "[epoch:28, iteration:84200] train loss : 0.1847 train accuracy : 0.9500\n",
            "[epoch:28, iteration:84400] train loss : 0.1611 train accuracy : 0.8500\n",
            "[epoch:28, iteration:84600] train loss : 0.1096 train accuracy : 0.9500\n",
            "[epoch:28, iteration:84800] train loss : 0.2791 train accuracy : 0.9500\n",
            "[epoch:28, iteration:85000] train loss : 0.0302 train accuracy : 1.0000\n",
            "[epoch:28, iteration:85200] train loss : 0.2485 train accuracy : 0.9500\n",
            "[epoch:28, iteration:85400] train loss : 0.0675 train accuracy : 1.0000\n",
            "[epoch:28, iteration:85600] train loss : 0.0994 train accuracy : 0.9500\n",
            "[epoch:28, iteration:85800] train loss : 0.1123 train accuracy : 0.9500\n",
            "[epoch:28, iteration:86000] train loss : 0.0273 train accuracy : 1.0000\n",
            "[epoch:28, iteration:86200] train loss : 0.3491 train accuracy : 0.8500\n",
            "[epoch:28, iteration:86400] train loss : 0.2310 train accuracy : 0.9000\n",
            "[epoch:28, iteration:86600] train loss : 0.1723 train accuracy : 0.9000\n",
            "[epoch:28, iteration:86800] train loss : 0.0401 train accuracy : 1.0000\n",
            "[epoch:28, iteration:87000] train loss : 0.0713 train accuracy : 1.0000\n",
            "[epoch:28, iteration:87000] test_loss : 0.2784 test accuracy : 0.9087\n",
            "[epoch:29, iteration:87200] train loss : 0.0870 train accuracy : 1.0000\n",
            "[epoch:29, iteration:87400] train loss : 0.0386 train accuracy : 1.0000\n",
            "[epoch:29, iteration:87600] train loss : 0.1439 train accuracy : 0.9500\n",
            "[epoch:29, iteration:87800] train loss : 0.6067 train accuracy : 0.8500\n",
            "[epoch:29, iteration:88000] train loss : 0.0704 train accuracy : 0.9500\n",
            "[epoch:29, iteration:88200] train loss : 0.1402 train accuracy : 0.9500\n",
            "[epoch:29, iteration:88400] train loss : 0.0335 train accuracy : 1.0000\n",
            "[epoch:29, iteration:88600] train loss : 0.0262 train accuracy : 1.0000\n",
            "[epoch:29, iteration:88800] train loss : 0.1115 train accuracy : 0.9500\n",
            "[epoch:29, iteration:89000] train loss : 0.2650 train accuracy : 0.8000\n",
            "[epoch:29, iteration:89200] train loss : 0.1043 train accuracy : 0.9500\n",
            "[epoch:29, iteration:89400] train loss : 0.0054 train accuracy : 1.0000\n",
            "[epoch:29, iteration:89600] train loss : 0.0128 train accuracy : 1.0000\n",
            "[epoch:29, iteration:89800] train loss : 0.2265 train accuracy : 0.8500\n",
            "[epoch:29, iteration:90000] train loss : 0.0481 train accuracy : 1.0000\n",
            "[epoch:29, iteration:90000] test_loss : 0.2769 test accuracy : 0.9082\n",
            "[epoch:30, iteration:90200] train loss : 0.1601 train accuracy : 0.9500\n",
            "[epoch:30, iteration:90400] train loss : 0.0898 train accuracy : 1.0000\n",
            "[epoch:30, iteration:90600] train loss : 0.0498 train accuracy : 1.0000\n",
            "[epoch:30, iteration:90800] train loss : 0.1144 train accuracy : 0.9500\n",
            "[epoch:30, iteration:91000] train loss : 0.1290 train accuracy : 0.9500\n",
            "[epoch:30, iteration:91200] train loss : 0.0669 train accuracy : 1.0000\n",
            "[epoch:30, iteration:91400] train loss : 0.0744 train accuracy : 1.0000\n",
            "[epoch:30, iteration:91600] train loss : 0.3614 train accuracy : 0.9500\n",
            "[epoch:30, iteration:91800] train loss : 0.1458 train accuracy : 0.9000\n",
            "[epoch:30, iteration:92000] train loss : 0.1342 train accuracy : 0.9000\n",
            "[epoch:30, iteration:92200] train loss : 0.0865 train accuracy : 1.0000\n",
            "[epoch:30, iteration:92400] train loss : 0.2251 train accuracy : 0.8500\n",
            "[epoch:30, iteration:92600] train loss : 0.0920 train accuracy : 0.9500\n",
            "[epoch:30, iteration:92800] train loss : 0.2288 train accuracy : 0.9500\n",
            "[epoch:30, iteration:93000] train loss : 0.1327 train accuracy : 0.9500\n",
            "[epoch:30, iteration:93000] test_loss : 0.2857 test accuracy : 0.9070\n",
            "[epoch:31, iteration:93200] train loss : 0.4376 train accuracy : 0.9500\n",
            "[epoch:31, iteration:93400] train loss : 0.1689 train accuracy : 0.9000\n",
            "[epoch:31, iteration:93600] train loss : 0.0606 train accuracy : 1.0000\n",
            "[epoch:31, iteration:93800] train loss : 0.2453 train accuracy : 0.9000\n",
            "[epoch:31, iteration:94000] train loss : 0.0329 train accuracy : 1.0000\n",
            "[epoch:31, iteration:94200] train loss : 0.1678 train accuracy : 0.9500\n",
            "[epoch:31, iteration:94400] train loss : 0.2310 train accuracy : 0.9500\n",
            "[epoch:31, iteration:94600] train loss : 0.1656 train accuracy : 0.9500\n",
            "[epoch:31, iteration:94800] train loss : 0.0709 train accuracy : 0.9500\n",
            "[epoch:31, iteration:95000] train loss : 0.0593 train accuracy : 1.0000\n",
            "[epoch:31, iteration:95200] train loss : 0.2663 train accuracy : 0.8500\n",
            "[epoch:31, iteration:95400] train loss : 0.5010 train accuracy : 0.7500\n",
            "[epoch:31, iteration:95600] train loss : 0.1863 train accuracy : 0.9000\n",
            "[epoch:31, iteration:95800] train loss : 0.2455 train accuracy : 0.9000\n",
            "[epoch:31, iteration:96000] train loss : 0.4920 train accuracy : 0.9000\n",
            "[epoch:31, iteration:96000] test_loss : 0.2944 test accuracy : 0.9079\n",
            "[epoch:32, iteration:96200] train loss : 0.0647 train accuracy : 0.9500\n",
            "[epoch:32, iteration:96400] train loss : 0.1046 train accuracy : 0.9000\n",
            "[epoch:32, iteration:96600] train loss : 0.0955 train accuracy : 0.9500\n",
            "[epoch:32, iteration:96800] train loss : 0.0887 train accuracy : 1.0000\n",
            "[epoch:32, iteration:97000] train loss : 0.0792 train accuracy : 0.9500\n",
            "[epoch:32, iteration:97200] train loss : 0.1341 train accuracy : 0.9500\n",
            "[epoch:32, iteration:97400] train loss : 0.0663 train accuracy : 1.0000\n",
            "[epoch:32, iteration:97600] train loss : 0.3880 train accuracy : 0.8000\n",
            "[epoch:32, iteration:97800] train loss : 0.3339 train accuracy : 0.9000\n",
            "[epoch:32, iteration:98000] train loss : 0.2016 train accuracy : 0.9500\n",
            "[epoch:32, iteration:98200] train loss : 0.3990 train accuracy : 0.8500\n",
            "[epoch:32, iteration:98400] train loss : 0.0900 train accuracy : 1.0000\n",
            "[epoch:32, iteration:98600] train loss : 0.0024 train accuracy : 1.0000\n",
            "[epoch:32, iteration:98800] train loss : 0.3077 train accuracy : 0.8000\n",
            "[epoch:32, iteration:99000] train loss : 0.1738 train accuracy : 0.9000\n",
            "[epoch:32, iteration:99000] test_loss : 0.2939 test accuracy : 0.9036\n",
            "[epoch:33, iteration:99200] train loss : 0.0694 train accuracy : 1.0000\n",
            "[epoch:33, iteration:99400] train loss : 0.0984 train accuracy : 0.9500\n",
            "[epoch:33, iteration:99600] train loss : 0.1199 train accuracy : 0.9500\n",
            "[epoch:33, iteration:99800] train loss : 0.0959 train accuracy : 0.9500\n",
            "[epoch:33, iteration:100000] train loss : 0.0799 train accuracy : 1.0000\n",
            "[epoch:33, iteration:100200] train loss : 0.0610 train accuracy : 1.0000\n",
            "[epoch:33, iteration:100400] train loss : 0.0918 train accuracy : 0.9500\n",
            "[epoch:33, iteration:100600] train loss : 0.1944 train accuracy : 0.9500\n",
            "[epoch:33, iteration:100800] train loss : 0.1896 train accuracy : 0.9000\n",
            "[epoch:33, iteration:101000] train loss : 0.0961 train accuracy : 0.9500\n",
            "[epoch:33, iteration:101200] train loss : 0.0921 train accuracy : 0.9500\n",
            "[epoch:33, iteration:101400] train loss : 0.0099 train accuracy : 1.0000\n",
            "[epoch:33, iteration:101600] train loss : 0.0998 train accuracy : 0.9500\n",
            "[epoch:33, iteration:101800] train loss : 0.0973 train accuracy : 0.9000\n",
            "[epoch:33, iteration:102000] train loss : 0.0317 train accuracy : 1.0000\n",
            "[epoch:33, iteration:102000] test_loss : 0.2898 test accuracy : 0.9094\n",
            "[epoch:34, iteration:102200] train loss : 0.0564 train accuracy : 0.9500\n",
            "[epoch:34, iteration:102400] train loss : 0.0603 train accuracy : 1.0000\n",
            "[epoch:34, iteration:102600] train loss : 0.2302 train accuracy : 0.9000\n",
            "[epoch:34, iteration:102800] train loss : 0.1220 train accuracy : 0.9500\n",
            "[epoch:34, iteration:103000] train loss : 0.0478 train accuracy : 1.0000\n",
            "[epoch:34, iteration:103200] train loss : 0.1585 train accuracy : 0.9500\n",
            "[epoch:34, iteration:103400] train loss : 0.3166 train accuracy : 0.9500\n",
            "[epoch:34, iteration:103600] train loss : 0.0502 train accuracy : 1.0000\n",
            "[epoch:34, iteration:103800] train loss : 0.0183 train accuracy : 1.0000\n",
            "[epoch:34, iteration:104000] train loss : 0.0251 train accuracy : 1.0000\n",
            "[epoch:34, iteration:104200] train loss : 0.1703 train accuracy : 0.9500\n",
            "[epoch:34, iteration:104400] train loss : 0.3266 train accuracy : 0.9500\n",
            "[epoch:34, iteration:104600] train loss : 0.2203 train accuracy : 0.8500\n",
            "[epoch:34, iteration:104800] train loss : 0.1184 train accuracy : 0.9500\n",
            "[epoch:34, iteration:105000] train loss : 0.2555 train accuracy : 0.8500\n",
            "[epoch:34, iteration:105000] test_loss : 0.2905 test accuracy : 0.9094\n",
            "[epoch:35, iteration:105200] train loss : 0.3314 train accuracy : 0.9000\n",
            "[epoch:35, iteration:105400] train loss : 0.0274 train accuracy : 1.0000\n",
            "[epoch:35, iteration:105600] train loss : 0.2010 train accuracy : 0.9000\n",
            "[epoch:35, iteration:105800] train loss : 0.0195 train accuracy : 1.0000\n",
            "[epoch:35, iteration:106000] train loss : 0.1229 train accuracy : 0.9500\n",
            "[epoch:35, iteration:106200] train loss : 0.1492 train accuracy : 0.9500\n",
            "[epoch:35, iteration:106400] train loss : 0.0604 train accuracy : 1.0000\n",
            "[epoch:35, iteration:106600] train loss : 0.0545 train accuracy : 1.0000\n",
            "[epoch:35, iteration:106800] train loss : 0.1544 train accuracy : 0.9000\n",
            "[epoch:35, iteration:107000] train loss : 0.1242 train accuracy : 0.9500\n",
            "[epoch:35, iteration:107200] train loss : 0.0711 train accuracy : 1.0000\n",
            "[epoch:35, iteration:107400] train loss : 0.1834 train accuracy : 0.9500\n",
            "[epoch:35, iteration:107600] train loss : 0.1459 train accuracy : 0.9000\n",
            "[epoch:35, iteration:107800] train loss : 0.1311 train accuracy : 1.0000\n",
            "[epoch:35, iteration:108000] train loss : 0.0897 train accuracy : 1.0000\n",
            "[epoch:35, iteration:108000] test_loss : 0.3010 test accuracy : 0.9084\n",
            "[epoch:36, iteration:108200] train loss : 0.2986 train accuracy : 0.9000\n",
            "[epoch:36, iteration:108400] train loss : 0.1404 train accuracy : 0.9500\n",
            "[epoch:36, iteration:108600] train loss : 0.0170 train accuracy : 1.0000\n",
            "[epoch:36, iteration:108800] train loss : 0.0476 train accuracy : 1.0000\n",
            "[epoch:36, iteration:109000] train loss : 0.0339 train accuracy : 1.0000\n",
            "[epoch:36, iteration:109200] train loss : 0.0445 train accuracy : 1.0000\n",
            "[epoch:36, iteration:109400] train loss : 0.1270 train accuracy : 0.9500\n",
            "[epoch:36, iteration:109600] train loss : 0.1804 train accuracy : 0.9500\n",
            "[epoch:36, iteration:109800] train loss : 0.2495 train accuracy : 0.9000\n",
            "[epoch:36, iteration:110000] train loss : 0.0716 train accuracy : 0.9500\n",
            "[epoch:36, iteration:110200] train loss : 0.1320 train accuracy : 0.9500\n",
            "[epoch:36, iteration:110400] train loss : 0.0740 train accuracy : 0.9500\n",
            "[epoch:36, iteration:110600] train loss : 0.0955 train accuracy : 0.9500\n",
            "[epoch:36, iteration:110800] train loss : 0.1039 train accuracy : 0.9500\n",
            "[epoch:36, iteration:111000] train loss : 0.0189 train accuracy : 1.0000\n",
            "[epoch:36, iteration:111000] test_loss : 0.3340 test accuracy : 0.9018\n",
            "[epoch:37, iteration:111200] train loss : 0.0066 train accuracy : 1.0000\n",
            "[epoch:37, iteration:111400] train loss : 0.0844 train accuracy : 1.0000\n",
            "[epoch:37, iteration:111600] train loss : 0.1864 train accuracy : 0.9000\n",
            "[epoch:37, iteration:111800] train loss : 0.1028 train accuracy : 0.9500\n",
            "[epoch:37, iteration:112000] train loss : 0.0739 train accuracy : 1.0000\n",
            "[epoch:37, iteration:112200] train loss : 0.0716 train accuracy : 0.9500\n",
            "[epoch:37, iteration:112400] train loss : 0.3419 train accuracy : 0.9000\n",
            "[epoch:37, iteration:112600] train loss : 0.1254 train accuracy : 0.9000\n",
            "[epoch:37, iteration:112800] train loss : 0.1124 train accuracy : 0.9500\n",
            "[epoch:37, iteration:113000] train loss : 0.0157 train accuracy : 1.0000\n",
            "[epoch:37, iteration:113200] train loss : 0.0430 train accuracy : 1.0000\n",
            "[epoch:37, iteration:113400] train loss : 0.1461 train accuracy : 0.9500\n",
            "[epoch:37, iteration:113600] train loss : 0.0441 train accuracy : 1.0000\n",
            "[epoch:37, iteration:113800] train loss : 0.4797 train accuracy : 0.8500\n",
            "[epoch:37, iteration:114000] train loss : 0.0884 train accuracy : 0.9500\n",
            "[epoch:37, iteration:114000] test_loss : 0.3080 test accuracy : 0.9095\n",
            "[epoch:38, iteration:114200] train loss : 0.2295 train accuracy : 0.8500\n",
            "[epoch:38, iteration:114400] train loss : 0.0676 train accuracy : 1.0000\n",
            "[epoch:38, iteration:114600] train loss : 0.2100 train accuracy : 0.8000\n",
            "[epoch:38, iteration:114800] train loss : 0.0337 train accuracy : 1.0000\n",
            "[epoch:38, iteration:115000] train loss : 0.0969 train accuracy : 1.0000\n",
            "[epoch:38, iteration:115200] train loss : 0.1531 train accuracy : 0.9500\n",
            "[epoch:38, iteration:115400] train loss : 0.0430 train accuracy : 1.0000\n",
            "[epoch:38, iteration:115600] train loss : 0.3804 train accuracy : 0.9500\n",
            "[epoch:38, iteration:115800] train loss : 0.0824 train accuracy : 0.9500\n",
            "[epoch:38, iteration:116000] train loss : 0.1025 train accuracy : 0.9500\n",
            "[epoch:38, iteration:116200] train loss : 0.1467 train accuracy : 0.9000\n",
            "[epoch:38, iteration:116400] train loss : 0.0099 train accuracy : 1.0000\n",
            "[epoch:38, iteration:116600] train loss : 0.0907 train accuracy : 0.9500\n",
            "[epoch:38, iteration:116800] train loss : 0.0162 train accuracy : 1.0000\n",
            "[epoch:38, iteration:117000] train loss : 0.0678 train accuracy : 0.9500\n",
            "[epoch:38, iteration:117000] test_loss : 0.3185 test accuracy : 0.9085\n",
            "[epoch:39, iteration:117200] train loss : 0.0288 train accuracy : 1.0000\n",
            "[epoch:39, iteration:117400] train loss : 0.0189 train accuracy : 1.0000\n",
            "[epoch:39, iteration:117600] train loss : 0.2558 train accuracy : 0.9000\n",
            "[epoch:39, iteration:117800] train loss : 0.0396 train accuracy : 1.0000\n",
            "[epoch:39, iteration:118000] train loss : 0.0699 train accuracy : 0.9500\n",
            "[epoch:39, iteration:118200] train loss : 0.0093 train accuracy : 1.0000\n",
            "[epoch:39, iteration:118400] train loss : 0.0081 train accuracy : 1.0000\n",
            "[epoch:39, iteration:118600] train loss : 0.0112 train accuracy : 1.0000\n",
            "[epoch:39, iteration:118800] train loss : 0.1732 train accuracy : 0.9000\n",
            "[epoch:39, iteration:119000] train loss : 0.0244 train accuracy : 1.0000\n",
            "[epoch:39, iteration:119200] train loss : 0.2121 train accuracy : 0.9000\n",
            "[epoch:39, iteration:119400] train loss : 0.0603 train accuracy : 1.0000\n",
            "[epoch:39, iteration:119600] train loss : 0.1800 train accuracy : 0.8500\n",
            "[epoch:39, iteration:119800] train loss : 0.1556 train accuracy : 0.9000\n",
            "[epoch:39, iteration:120000] train loss : 0.0234 train accuracy : 1.0000\n",
            "[epoch:39, iteration:120000] test_loss : 0.3155 test accuracy : 0.9107\n",
            "[epoch:40, iteration:120200] train loss : 0.2601 train accuracy : 0.9000\n",
            "[epoch:40, iteration:120400] train loss : 0.0347 train accuracy : 1.0000\n",
            "[epoch:40, iteration:120600] train loss : 0.0946 train accuracy : 0.9500\n",
            "[epoch:40, iteration:120800] train loss : 0.0081 train accuracy : 1.0000\n",
            "[epoch:40, iteration:121000] train loss : 0.1773 train accuracy : 0.9000\n",
            "[epoch:40, iteration:121200] train loss : 0.0565 train accuracy : 0.9500\n",
            "[epoch:40, iteration:121400] train loss : 0.1146 train accuracy : 0.9500\n",
            "[epoch:40, iteration:121600] train loss : 0.0267 train accuracy : 1.0000\n",
            "[epoch:40, iteration:121800] train loss : 0.0457 train accuracy : 1.0000\n",
            "[epoch:40, iteration:122000] train loss : 0.2137 train accuracy : 0.9500\n",
            "[epoch:40, iteration:122200] train loss : 0.0218 train accuracy : 1.0000\n",
            "[epoch:40, iteration:122400] train loss : 0.0186 train accuracy : 1.0000\n",
            "[epoch:40, iteration:122600] train loss : 0.1841 train accuracy : 0.9500\n",
            "[epoch:40, iteration:122800] train loss : 0.0904 train accuracy : 0.9500\n",
            "[epoch:40, iteration:123000] train loss : 0.1045 train accuracy : 0.9500\n",
            "[epoch:40, iteration:123000] test_loss : 0.3365 test accuracy : 0.9068\n",
            "[epoch:41, iteration:123200] train loss : 0.0870 train accuracy : 0.9500\n",
            "[epoch:41, iteration:123400] train loss : 0.0356 train accuracy : 1.0000\n",
            "[epoch:41, iteration:123600] train loss : 0.0973 train accuracy : 0.9500\n",
            "[epoch:41, iteration:123800] train loss : 0.2522 train accuracy : 0.9000\n",
            "[epoch:41, iteration:124000] train loss : 0.1025 train accuracy : 0.9500\n",
            "[epoch:41, iteration:124200] train loss : 0.0148 train accuracy : 1.0000\n",
            "[epoch:41, iteration:124400] train loss : 0.0330 train accuracy : 1.0000\n",
            "[epoch:41, iteration:124600] train loss : 0.0311 train accuracy : 1.0000\n",
            "[epoch:41, iteration:124800] train loss : 0.0710 train accuracy : 0.9500\n",
            "[epoch:41, iteration:125000] train loss : 0.0261 train accuracy : 1.0000\n",
            "[epoch:41, iteration:125200] train loss : 0.0479 train accuracy : 0.9500\n",
            "[epoch:41, iteration:125400] train loss : 0.0361 train accuracy : 1.0000\n",
            "[epoch:41, iteration:125600] train loss : 0.1539 train accuracy : 0.9000\n",
            "[epoch:41, iteration:125800] train loss : 0.0969 train accuracy : 0.9500\n",
            "[epoch:41, iteration:126000] train loss : 0.0914 train accuracy : 0.9500\n",
            "[epoch:41, iteration:126000] test_loss : 0.3284 test accuracy : 0.9096\n",
            "[epoch:42, iteration:126200] train loss : 0.1900 train accuracy : 0.9500\n",
            "[epoch:42, iteration:126400] train loss : 0.2085 train accuracy : 0.9500\n",
            "[epoch:42, iteration:126600] train loss : 0.3928 train accuracy : 0.8000\n",
            "[epoch:42, iteration:126800] train loss : 0.0644 train accuracy : 0.9500\n",
            "[epoch:42, iteration:127000] train loss : 0.0031 train accuracy : 1.0000\n",
            "[epoch:42, iteration:127200] train loss : 0.0568 train accuracy : 1.0000\n",
            "[epoch:42, iteration:127400] train loss : 0.0315 train accuracy : 1.0000\n",
            "[epoch:42, iteration:127600] train loss : 0.0204 train accuracy : 1.0000\n",
            "[epoch:42, iteration:127800] train loss : 0.1610 train accuracy : 0.9000\n",
            "[epoch:42, iteration:128000] train loss : 0.0642 train accuracy : 0.9500\n",
            "[epoch:42, iteration:128200] train loss : 0.1031 train accuracy : 0.9500\n",
            "[epoch:42, iteration:128400] train loss : 0.0210 train accuracy : 1.0000\n",
            "[epoch:42, iteration:128600] train loss : 0.1293 train accuracy : 0.9500\n",
            "[epoch:42, iteration:128800] train loss : 0.0223 train accuracy : 1.0000\n",
            "[epoch:42, iteration:129000] train loss : 0.0366 train accuracy : 1.0000\n",
            "[epoch:42, iteration:129000] test_loss : 0.3355 test accuracy : 0.9079\n",
            "[epoch:43, iteration:129200] train loss : 0.2026 train accuracy : 0.9000\n",
            "[epoch:43, iteration:129400] train loss : 0.0479 train accuracy : 1.0000\n",
            "[epoch:43, iteration:129600] train loss : 0.1267 train accuracy : 0.9500\n",
            "[epoch:43, iteration:129800] train loss : 0.0469 train accuracy : 1.0000\n",
            "[epoch:43, iteration:130000] train loss : 0.0864 train accuracy : 0.9500\n",
            "[epoch:43, iteration:130200] train loss : 0.0257 train accuracy : 1.0000\n",
            "[epoch:43, iteration:130400] train loss : 0.0624 train accuracy : 1.0000\n",
            "[epoch:43, iteration:130600] train loss : 0.1633 train accuracy : 0.9500\n",
            "[epoch:43, iteration:130800] train loss : 0.1212 train accuracy : 0.9500\n",
            "[epoch:43, iteration:131000] train loss : 0.2855 train accuracy : 0.9000\n",
            "[epoch:43, iteration:131200] train loss : 0.0712 train accuracy : 0.9500\n",
            "[epoch:43, iteration:131400] train loss : 0.1527 train accuracy : 0.9500\n",
            "[epoch:43, iteration:131600] train loss : 0.0729 train accuracy : 0.9500\n",
            "[epoch:43, iteration:131800] train loss : 0.0976 train accuracy : 0.9500\n",
            "[epoch:43, iteration:132000] train loss : 0.1862 train accuracy : 0.8500\n",
            "[epoch:43, iteration:132000] test_loss : 0.3401 test accuracy : 0.9102\n",
            "[epoch:44, iteration:132200] train loss : 0.0763 train accuracy : 0.9500\n",
            "[epoch:44, iteration:132400] train loss : 0.0247 train accuracy : 1.0000\n",
            "[epoch:44, iteration:132600] train loss : 0.0085 train accuracy : 1.0000\n",
            "[epoch:44, iteration:132800] train loss : 0.0473 train accuracy : 1.0000\n",
            "[epoch:44, iteration:133000] train loss : 0.0396 train accuracy : 1.0000\n",
            "[epoch:44, iteration:133200] train loss : 0.1916 train accuracy : 0.8500\n",
            "[epoch:44, iteration:133400] train loss : 0.0588 train accuracy : 0.9500\n",
            "[epoch:44, iteration:133600] train loss : 0.1892 train accuracy : 0.9500\n",
            "[epoch:44, iteration:133800] train loss : 0.0721 train accuracy : 0.9500\n",
            "[epoch:44, iteration:134000] train loss : 0.0207 train accuracy : 1.0000\n",
            "[epoch:44, iteration:134200] train loss : 0.0553 train accuracy : 1.0000\n",
            "[epoch:44, iteration:134400] train loss : 0.0480 train accuracy : 1.0000\n",
            "[epoch:44, iteration:134600] train loss : 0.0051 train accuracy : 1.0000\n",
            "[epoch:44, iteration:134800] train loss : 0.0722 train accuracy : 0.9500\n",
            "[epoch:44, iteration:135000] train loss : 0.0919 train accuracy : 0.9500\n",
            "[epoch:44, iteration:135000] test_loss : 0.3483 test accuracy : 0.9097\n",
            "[epoch:45, iteration:135200] train loss : 0.3006 train accuracy : 0.9500\n",
            "[epoch:45, iteration:135400] train loss : 0.0239 train accuracy : 1.0000\n",
            "[epoch:45, iteration:135600] train loss : 0.0391 train accuracy : 1.0000\n",
            "[epoch:45, iteration:135800] train loss : 0.0547 train accuracy : 1.0000\n",
            "[epoch:45, iteration:136000] train loss : 0.0584 train accuracy : 0.9500\n",
            "[epoch:45, iteration:136200] train loss : 0.0464 train accuracy : 0.9500\n",
            "[epoch:45, iteration:136400] train loss : 0.1327 train accuracy : 0.9500\n",
            "[epoch:45, iteration:136600] train loss : 0.0822 train accuracy : 1.0000\n",
            "[epoch:45, iteration:136800] train loss : 0.0151 train accuracy : 1.0000\n",
            "[epoch:45, iteration:137000] train loss : 0.0258 train accuracy : 1.0000\n",
            "[epoch:45, iteration:137200] train loss : 0.0853 train accuracy : 0.9000\n",
            "[epoch:45, iteration:137400] train loss : 0.0691 train accuracy : 1.0000\n",
            "[epoch:45, iteration:137600] train loss : 0.0686 train accuracy : 1.0000\n",
            "[epoch:45, iteration:137800] train loss : 0.0503 train accuracy : 0.9500\n",
            "[epoch:45, iteration:138000] train loss : 0.0566 train accuracy : 1.0000\n",
            "[epoch:45, iteration:138000] test_loss : 0.3589 test accuracy : 0.9075\n",
            "[epoch:46, iteration:138200] train loss : 0.0632 train accuracy : 0.9500\n",
            "[epoch:46, iteration:138400] train loss : 0.1459 train accuracy : 0.9000\n",
            "[epoch:46, iteration:138600] train loss : 0.0985 train accuracy : 1.0000\n",
            "[epoch:46, iteration:138800] train loss : 0.0034 train accuracy : 1.0000\n",
            "[epoch:46, iteration:139000] train loss : 0.0621 train accuracy : 0.9500\n",
            "[epoch:46, iteration:139200] train loss : 0.0436 train accuracy : 1.0000\n",
            "[epoch:46, iteration:139400] train loss : 0.0200 train accuracy : 1.0000\n",
            "[epoch:46, iteration:139600] train loss : 0.0091 train accuracy : 1.0000\n",
            "[epoch:46, iteration:139800] train loss : 0.0492 train accuracy : 1.0000\n",
            "[epoch:46, iteration:140000] train loss : 0.0869 train accuracy : 0.9500\n",
            "[epoch:46, iteration:140200] train loss : 0.1174 train accuracy : 0.9500\n",
            "[epoch:46, iteration:140400] train loss : 0.0364 train accuracy : 1.0000\n",
            "[epoch:46, iteration:140600] train loss : 0.1167 train accuracy : 0.9500\n",
            "[epoch:46, iteration:140800] train loss : 0.1355 train accuracy : 0.9500\n",
            "[epoch:46, iteration:141000] train loss : 0.0946 train accuracy : 0.9500\n",
            "[epoch:46, iteration:141000] test_loss : 0.3647 test accuracy : 0.9054\n",
            "[epoch:47, iteration:141200] train loss : 0.1190 train accuracy : 0.9500\n",
            "[epoch:47, iteration:141400] train loss : 0.0553 train accuracy : 1.0000\n",
            "[epoch:47, iteration:141600] train loss : 0.0696 train accuracy : 1.0000\n",
            "[epoch:47, iteration:141800] train loss : 0.0050 train accuracy : 1.0000\n",
            "[epoch:47, iteration:142000] train loss : 0.0047 train accuracy : 1.0000\n",
            "[epoch:47, iteration:142200] train loss : 0.1061 train accuracy : 0.9500\n",
            "[epoch:47, iteration:142400] train loss : 0.0745 train accuracy : 0.9500\n",
            "[epoch:47, iteration:142600] train loss : 0.2714 train accuracy : 0.9500\n",
            "[epoch:47, iteration:142800] train loss : 0.0687 train accuracy : 0.9500\n",
            "[epoch:47, iteration:143000] train loss : 0.0391 train accuracy : 1.0000\n",
            "[epoch:47, iteration:143200] train loss : 0.3194 train accuracy : 0.9500\n",
            "[epoch:47, iteration:143400] train loss : 0.1680 train accuracy : 0.9000\n",
            "[epoch:47, iteration:143600] train loss : 0.2137 train accuracy : 0.9500\n",
            "[epoch:47, iteration:143800] train loss : 0.1508 train accuracy : 0.9500\n",
            "[epoch:47, iteration:144000] train loss : 0.0825 train accuracy : 0.9500\n",
            "[epoch:47, iteration:144000] test_loss : 0.3747 test accuracy : 0.9061\n",
            "[epoch:48, iteration:144200] train loss : 0.0310 train accuracy : 1.0000\n",
            "[epoch:48, iteration:144400] train loss : 0.0180 train accuracy : 1.0000\n",
            "[epoch:48, iteration:144600] train loss : 0.0123 train accuracy : 1.0000\n",
            "[epoch:48, iteration:144800] train loss : 0.0238 train accuracy : 1.0000\n",
            "[epoch:48, iteration:145000] train loss : 0.0141 train accuracy : 1.0000\n",
            "[epoch:48, iteration:145200] train loss : 0.0068 train accuracy : 1.0000\n",
            "[epoch:48, iteration:145400] train loss : 0.1214 train accuracy : 0.9500\n",
            "[epoch:48, iteration:145600] train loss : 0.0906 train accuracy : 1.0000\n",
            "[epoch:48, iteration:145800] train loss : 0.0102 train accuracy : 1.0000\n",
            "[epoch:48, iteration:146000] train loss : 0.0965 train accuracy : 0.9500\n",
            "[epoch:48, iteration:146200] train loss : 0.1322 train accuracy : 0.9000\n",
            "[epoch:48, iteration:146400] train loss : 0.0578 train accuracy : 1.0000\n",
            "[epoch:48, iteration:146600] train loss : 0.1068 train accuracy : 0.9000\n",
            "[epoch:48, iteration:146800] train loss : 0.3605 train accuracy : 0.9500\n",
            "[epoch:48, iteration:147000] train loss : 0.3163 train accuracy : 0.9000\n",
            "[epoch:48, iteration:147000] test_loss : 0.3804 test accuracy : 0.9082\n",
            "[epoch:49, iteration:147200] train loss : 0.0285 train accuracy : 1.0000\n",
            "[epoch:49, iteration:147400] train loss : 0.0983 train accuracy : 0.9000\n",
            "[epoch:49, iteration:147600] train loss : 0.1853 train accuracy : 0.9500\n",
            "[epoch:49, iteration:147800] train loss : 0.0232 train accuracy : 1.0000\n",
            "[epoch:49, iteration:148000] train loss : 0.0792 train accuracy : 0.9500\n",
            "[epoch:49, iteration:148200] train loss : 0.0387 train accuracy : 1.0000\n",
            "[epoch:49, iteration:148400] train loss : 0.1148 train accuracy : 0.9500\n",
            "[epoch:49, iteration:148600] train loss : 0.0106 train accuracy : 1.0000\n",
            "[epoch:49, iteration:148800] train loss : 0.0572 train accuracy : 0.9500\n",
            "[epoch:49, iteration:149000] train loss : 0.0993 train accuracy : 0.9500\n",
            "[epoch:49, iteration:149200] train loss : 0.1414 train accuracy : 0.9000\n",
            "[epoch:49, iteration:149400] train loss : 0.1238 train accuracy : 0.9500\n",
            "[epoch:49, iteration:149600] train loss : 0.0397 train accuracy : 1.0000\n",
            "[epoch:49, iteration:149800] train loss : 0.0054 train accuracy : 1.0000\n",
            "[epoch:49, iteration:150000] train loss : 0.0215 train accuracy : 1.0000\n",
            "[epoch:49, iteration:150000] test_loss : 0.3864 test accuracy : 0.9071\n",
            "[epoch:50, iteration:150200] train loss : 0.0445 train accuracy : 1.0000\n",
            "[epoch:50, iteration:150400] train loss : 0.0367 train accuracy : 1.0000\n",
            "[epoch:50, iteration:150600] train loss : 0.0116 train accuracy : 1.0000\n",
            "[epoch:50, iteration:150800] train loss : 0.0381 train accuracy : 1.0000\n",
            "[epoch:50, iteration:151000] train loss : 0.0281 train accuracy : 1.0000\n",
            "[epoch:50, iteration:151200] train loss : 0.0368 train accuracy : 1.0000\n",
            "[epoch:50, iteration:151400] train loss : 0.1093 train accuracy : 0.9500\n",
            "[epoch:50, iteration:151600] train loss : 0.2120 train accuracy : 0.8500\n",
            "[epoch:50, iteration:151800] train loss : 0.1854 train accuracy : 0.9500\n",
            "[epoch:50, iteration:152000] train loss : 0.1814 train accuracy : 0.8500\n",
            "[epoch:50, iteration:152200] train loss : 0.0279 train accuracy : 1.0000\n",
            "[epoch:50, iteration:152400] train loss : 0.0949 train accuracy : 0.9500\n",
            "[epoch:50, iteration:152600] train loss : 0.0243 train accuracy : 1.0000\n",
            "[epoch:50, iteration:152800] train loss : 0.2005 train accuracy : 0.9000\n",
            "[epoch:50, iteration:153000] train loss : 0.0665 train accuracy : 0.9500\n",
            "[epoch:50, iteration:153000] test_loss : 0.3999 test accuracy : 0.9050\n",
            "[epoch:51, iteration:153200] train loss : 0.0318 train accuracy : 1.0000\n",
            "[epoch:51, iteration:153400] train loss : 0.1355 train accuracy : 0.9000\n",
            "[epoch:51, iteration:153600] train loss : 0.0187 train accuracy : 1.0000\n",
            "[epoch:51, iteration:153800] train loss : 0.0152 train accuracy : 1.0000\n",
            "[epoch:51, iteration:154000] train loss : 0.0412 train accuracy : 1.0000\n",
            "[epoch:51, iteration:154200] train loss : 0.0021 train accuracy : 1.0000\n",
            "[epoch:51, iteration:154400] train loss : 0.0971 train accuracy : 0.9500\n",
            "[epoch:51, iteration:154600] train loss : 0.1132 train accuracy : 0.9500\n",
            "[epoch:51, iteration:154800] train loss : 0.0012 train accuracy : 1.0000\n",
            "[epoch:51, iteration:155000] train loss : 0.1864 train accuracy : 0.9000\n",
            "[epoch:51, iteration:155200] train loss : 0.0190 train accuracy : 1.0000\n",
            "[epoch:51, iteration:155400] train loss : 0.1791 train accuracy : 0.9500\n",
            "[epoch:51, iteration:155600] train loss : 0.0369 train accuracy : 1.0000\n",
            "[epoch:51, iteration:155800] train loss : 0.2276 train accuracy : 0.8000\n",
            "[epoch:51, iteration:156000] train loss : 0.0799 train accuracy : 0.9500\n",
            "[epoch:51, iteration:156000] test_loss : 0.4141 test accuracy : 0.9060\n",
            "[epoch:52, iteration:156200] train loss : 0.0592 train accuracy : 1.0000\n",
            "[epoch:52, iteration:156400] train loss : 0.0343 train accuracy : 1.0000\n",
            "[epoch:52, iteration:156600] train loss : 0.0151 train accuracy : 1.0000\n",
            "[epoch:52, iteration:156800] train loss : 0.2594 train accuracy : 0.9500\n",
            "[epoch:52, iteration:157000] train loss : 0.0825 train accuracy : 0.9500\n",
            "[epoch:52, iteration:157200] train loss : 0.1321 train accuracy : 0.9500\n",
            "[epoch:52, iteration:157400] train loss : 0.0409 train accuracy : 1.0000\n",
            "[epoch:52, iteration:157600] train loss : 0.0119 train accuracy : 1.0000\n",
            "[epoch:52, iteration:157800] train loss : 0.0321 train accuracy : 1.0000\n",
            "[epoch:52, iteration:158000] train loss : 0.0435 train accuracy : 1.0000\n",
            "[epoch:52, iteration:158200] train loss : 0.0114 train accuracy : 1.0000\n",
            "[epoch:52, iteration:158400] train loss : 0.0198 train accuracy : 1.0000\n",
            "[epoch:52, iteration:158600] train loss : 0.1085 train accuracy : 0.9500\n",
            "[epoch:52, iteration:158800] train loss : 0.0051 train accuracy : 1.0000\n",
            "[epoch:52, iteration:159000] train loss : 0.0510 train accuracy : 0.9500\n",
            "[epoch:52, iteration:159000] test_loss : 0.4140 test accuracy : 0.9073\n",
            "[epoch:53, iteration:159200] train loss : 0.0078 train accuracy : 1.0000\n",
            "[epoch:53, iteration:159400] train loss : 0.0212 train accuracy : 1.0000\n",
            "[epoch:53, iteration:159600] train loss : 0.1346 train accuracy : 0.9000\n",
            "[epoch:53, iteration:159800] train loss : 0.0764 train accuracy : 0.9500\n",
            "[epoch:53, iteration:160000] train loss : 0.0378 train accuracy : 1.0000\n",
            "[epoch:53, iteration:160200] train loss : 0.0448 train accuracy : 0.9500\n",
            "[epoch:53, iteration:160400] train loss : 0.0071 train accuracy : 1.0000\n",
            "[epoch:53, iteration:160600] train loss : 0.0134 train accuracy : 1.0000\n",
            "[epoch:53, iteration:160800] train loss : 0.0136 train accuracy : 1.0000\n",
            "[epoch:53, iteration:161000] train loss : 0.1101 train accuracy : 0.9500\n",
            "[epoch:53, iteration:161200] train loss : 0.0527 train accuracy : 1.0000\n",
            "[epoch:53, iteration:161400] train loss : 0.1643 train accuracy : 0.9500\n",
            "[epoch:53, iteration:161600] train loss : 0.0338 train accuracy : 1.0000\n",
            "[epoch:53, iteration:161800] train loss : 0.0243 train accuracy : 1.0000\n",
            "[epoch:53, iteration:162000] train loss : 0.0790 train accuracy : 0.9500\n",
            "[epoch:53, iteration:162000] test_loss : 0.4230 test accuracy : 0.9055\n",
            "[epoch:54, iteration:162200] train loss : 0.0716 train accuracy : 0.9500\n",
            "[epoch:54, iteration:162400] train loss : 0.0402 train accuracy : 1.0000\n",
            "[epoch:54, iteration:162600] train loss : 0.0005 train accuracy : 1.0000\n",
            "[epoch:54, iteration:162800] train loss : 0.0971 train accuracy : 0.9500\n",
            "[epoch:54, iteration:163000] train loss : 0.0493 train accuracy : 0.9500\n",
            "[epoch:54, iteration:163200] train loss : 0.2113 train accuracy : 0.9500\n",
            "[epoch:54, iteration:163400] train loss : 0.0892 train accuracy : 0.9500\n",
            "[epoch:54, iteration:163600] train loss : 0.0113 train accuracy : 1.0000\n",
            "[epoch:54, iteration:163800] train loss : 0.0144 train accuracy : 1.0000\n",
            "[epoch:54, iteration:164000] train loss : 0.1215 train accuracy : 0.9500\n",
            "[epoch:54, iteration:164200] train loss : 0.1866 train accuracy : 0.9500\n",
            "[epoch:54, iteration:164400] train loss : 0.0789 train accuracy : 0.9500\n",
            "[epoch:54, iteration:164600] train loss : 0.0020 train accuracy : 1.0000\n",
            "[epoch:54, iteration:164800] train loss : 0.1752 train accuracy : 0.9000\n",
            "[epoch:54, iteration:165000] train loss : 0.0221 train accuracy : 1.0000\n",
            "[epoch:54, iteration:165000] test_loss : 0.4267 test accuracy : 0.9071\n",
            "[epoch:55, iteration:165200] train loss : 0.0221 train accuracy : 1.0000\n",
            "[epoch:55, iteration:165400] train loss : 0.0032 train accuracy : 1.0000\n",
            "[epoch:55, iteration:165600] train loss : 0.0071 train accuracy : 1.0000\n",
            "[epoch:55, iteration:165800] train loss : 0.1475 train accuracy : 0.9500\n",
            "[epoch:55, iteration:166000] train loss : 0.1264 train accuracy : 0.9000\n",
            "[epoch:55, iteration:166200] train loss : 0.2771 train accuracy : 0.9000\n",
            "[epoch:55, iteration:166400] train loss : 0.0180 train accuracy : 1.0000\n",
            "[epoch:55, iteration:166600] train loss : 0.0090 train accuracy : 1.0000\n",
            "[epoch:55, iteration:166800] train loss : 0.0326 train accuracy : 1.0000\n",
            "[epoch:55, iteration:167000] train loss : 0.0654 train accuracy : 1.0000\n",
            "[epoch:55, iteration:167200] train loss : 0.1509 train accuracy : 0.9500\n",
            "[epoch:55, iteration:167400] train loss : 0.0837 train accuracy : 1.0000\n",
            "[epoch:55, iteration:167600] train loss : 0.0433 train accuracy : 1.0000\n",
            "[epoch:55, iteration:167800] train loss : 0.1690 train accuracy : 0.9500\n",
            "[epoch:55, iteration:168000] train loss : 0.1710 train accuracy : 0.9000\n",
            "[epoch:55, iteration:168000] test_loss : 0.4463 test accuracy : 0.9043\n",
            "[epoch:56, iteration:168200] train loss : 0.0014 train accuracy : 1.0000\n",
            "[epoch:56, iteration:168400] train loss : 0.0223 train accuracy : 1.0000\n",
            "[epoch:56, iteration:168600] train loss : 0.0402 train accuracy : 1.0000\n",
            "[epoch:56, iteration:168800] train loss : 0.2508 train accuracy : 0.9500\n",
            "[epoch:56, iteration:169000] train loss : 0.0092 train accuracy : 1.0000\n",
            "[epoch:56, iteration:169200] train loss : 0.1238 train accuracy : 0.9500\n",
            "[epoch:56, iteration:169400] train loss : 0.0352 train accuracy : 1.0000\n",
            "[epoch:56, iteration:169600] train loss : 0.0042 train accuracy : 1.0000\n",
            "[epoch:56, iteration:169800] train loss : 0.0460 train accuracy : 1.0000\n",
            "[epoch:56, iteration:170000] train loss : 0.0968 train accuracy : 0.9500\n",
            "[epoch:56, iteration:170200] train loss : 0.0072 train accuracy : 1.0000\n",
            "[epoch:56, iteration:170400] train loss : 0.1610 train accuracy : 0.9500\n",
            "[epoch:56, iteration:170600] train loss : 0.1219 train accuracy : 0.9000\n",
            "[epoch:56, iteration:170800] train loss : 0.0040 train accuracy : 1.0000\n",
            "[epoch:56, iteration:171000] train loss : 0.0180 train accuracy : 1.0000\n",
            "[epoch:56, iteration:171000] test_loss : 0.4668 test accuracy : 0.9033\n",
            "[epoch:57, iteration:171200] train loss : 0.1015 train accuracy : 0.9500\n",
            "[epoch:57, iteration:171400] train loss : 0.1313 train accuracy : 0.9500\n",
            "[epoch:57, iteration:171600] train loss : 0.0264 train accuracy : 1.0000\n",
            "[epoch:57, iteration:171800] train loss : 0.0104 train accuracy : 1.0000\n",
            "[epoch:57, iteration:172000] train loss : 0.0406 train accuracy : 0.9500\n",
            "[epoch:57, iteration:172200] train loss : 0.0002 train accuracy : 1.0000\n",
            "[epoch:57, iteration:172400] train loss : 0.1411 train accuracy : 0.9000\n",
            "[epoch:57, iteration:172600] train loss : 0.0052 train accuracy : 1.0000\n",
            "[epoch:57, iteration:172800] train loss : 0.0560 train accuracy : 1.0000\n",
            "[epoch:57, iteration:173000] train loss : 0.0106 train accuracy : 1.0000\n",
            "[epoch:57, iteration:173200] train loss : 0.1553 train accuracy : 0.9500\n",
            "[epoch:57, iteration:173400] train loss : 0.0612 train accuracy : 0.9500\n",
            "[epoch:57, iteration:173600] train loss : 0.0733 train accuracy : 0.9500\n",
            "[epoch:57, iteration:173800] train loss : 0.1694 train accuracy : 0.9500\n",
            "[epoch:57, iteration:174000] train loss : 0.0617 train accuracy : 1.0000\n",
            "[epoch:57, iteration:174000] test_loss : 0.4640 test accuracy : 0.9047\n",
            "[epoch:58, iteration:174200] train loss : 0.0260 train accuracy : 1.0000\n",
            "[epoch:58, iteration:174400] train loss : 0.0447 train accuracy : 0.9500\n",
            "[epoch:58, iteration:174600] train loss : 0.0081 train accuracy : 1.0000\n",
            "[epoch:58, iteration:174800] train loss : 0.0618 train accuracy : 0.9500\n",
            "[epoch:58, iteration:175000] train loss : 0.0119 train accuracy : 1.0000\n",
            "[epoch:58, iteration:175200] train loss : 0.0436 train accuracy : 0.9500\n",
            "[epoch:58, iteration:175400] train loss : 0.1508 train accuracy : 0.9500\n",
            "[epoch:58, iteration:175600] train loss : 0.1407 train accuracy : 0.9500\n",
            "[epoch:58, iteration:175800] train loss : 0.1744 train accuracy : 0.9000\n",
            "[epoch:58, iteration:176000] train loss : 0.0235 train accuracy : 1.0000\n",
            "[epoch:58, iteration:176200] train loss : 0.2933 train accuracy : 0.9000\n",
            "[epoch:58, iteration:176400] train loss : 0.1071 train accuracy : 0.9000\n",
            "[epoch:58, iteration:176600] train loss : 0.0720 train accuracy : 0.9500\n",
            "[epoch:58, iteration:176800] train loss : 0.1469 train accuracy : 0.9500\n",
            "[epoch:58, iteration:177000] train loss : 0.0235 train accuracy : 1.0000\n",
            "[epoch:58, iteration:177000] test_loss : 0.4561 test accuracy : 0.9059\n",
            "[epoch:59, iteration:177200] train loss : 0.0261 train accuracy : 1.0000\n",
            "[epoch:59, iteration:177400] train loss : 0.0178 train accuracy : 1.0000\n",
            "[epoch:59, iteration:177600] train loss : 0.0338 train accuracy : 1.0000\n",
            "[epoch:59, iteration:177800] train loss : 0.0078 train accuracy : 1.0000\n",
            "[epoch:59, iteration:178000] train loss : 0.0419 train accuracy : 1.0000\n",
            "[epoch:59, iteration:178200] train loss : 0.0096 train accuracy : 1.0000\n",
            "[epoch:59, iteration:178400] train loss : 0.0254 train accuracy : 1.0000\n",
            "[epoch:59, iteration:178600] train loss : 0.0537 train accuracy : 1.0000\n",
            "[epoch:59, iteration:178800] train loss : 0.0654 train accuracy : 0.9500\n",
            "[epoch:59, iteration:179000] train loss : 0.1497 train accuracy : 0.9500\n",
            "[epoch:59, iteration:179200] train loss : 0.0497 train accuracy : 1.0000\n",
            "[epoch:59, iteration:179400] train loss : 0.0229 train accuracy : 1.0000\n",
            "[epoch:59, iteration:179600] train loss : 0.0187 train accuracy : 1.0000\n",
            "[epoch:59, iteration:179800] train loss : 0.0264 train accuracy : 1.0000\n",
            "[epoch:59, iteration:180000] train loss : 0.0864 train accuracy : 0.9500\n",
            "[epoch:59, iteration:180000] test_loss : 0.4894 test accuracy : 0.9041\n",
            "[epoch:60, iteration:180200] train loss : 0.0048 train accuracy : 1.0000\n",
            "[epoch:60, iteration:180400] train loss : 0.1290 train accuracy : 0.9500\n",
            "[epoch:60, iteration:180600] train loss : 0.0767 train accuracy : 0.9500\n",
            "[epoch:60, iteration:180800] train loss : 0.0654 train accuracy : 0.9500\n",
            "[epoch:60, iteration:181000] train loss : 0.0271 train accuracy : 1.0000\n",
            "[epoch:60, iteration:181200] train loss : 0.2151 train accuracy : 0.9000\n",
            "[epoch:60, iteration:181400] train loss : 0.0396 train accuracy : 1.0000\n",
            "[epoch:60, iteration:181600] train loss : 0.0266 train accuracy : 1.0000\n",
            "[epoch:60, iteration:181800] train loss : 0.0283 train accuracy : 1.0000\n",
            "[epoch:60, iteration:182000] train loss : 0.0896 train accuracy : 0.9500\n",
            "[epoch:60, iteration:182200] train loss : 0.1204 train accuracy : 0.9500\n",
            "[epoch:60, iteration:182400] train loss : 0.0020 train accuracy : 1.0000\n",
            "[epoch:60, iteration:182600] train loss : 0.0484 train accuracy : 1.0000\n",
            "[epoch:60, iteration:182800] train loss : 0.0156 train accuracy : 1.0000\n",
            "[epoch:60, iteration:183000] train loss : 0.0113 train accuracy : 1.0000\n",
            "[epoch:60, iteration:183000] test_loss : 0.5059 test accuracy : 0.9038\n",
            "[epoch:61, iteration:183200] train loss : 0.0055 train accuracy : 1.0000\n",
            "[epoch:61, iteration:183400] train loss : 0.0043 train accuracy : 1.0000\n",
            "[epoch:61, iteration:183600] train loss : 0.0727 train accuracy : 0.9500\n",
            "[epoch:61, iteration:183800] train loss : 0.0293 train accuracy : 1.0000\n",
            "[epoch:61, iteration:184000] train loss : 0.0201 train accuracy : 1.0000\n",
            "[epoch:61, iteration:184200] train loss : 0.0090 train accuracy : 1.0000\n",
            "[epoch:61, iteration:184400] train loss : 0.1315 train accuracy : 0.9500\n",
            "[epoch:61, iteration:184600] train loss : 0.0219 train accuracy : 1.0000\n",
            "[epoch:61, iteration:184800] train loss : 0.0280 train accuracy : 1.0000\n",
            "[epoch:61, iteration:185000] train loss : 0.0054 train accuracy : 1.0000\n",
            "[epoch:61, iteration:185200] train loss : 0.1455 train accuracy : 0.9500\n",
            "[epoch:61, iteration:185400] train loss : 0.0429 train accuracy : 0.9500\n",
            "[epoch:61, iteration:185600] train loss : 0.0691 train accuracy : 0.9500\n",
            "[epoch:61, iteration:185800] train loss : 0.2008 train accuracy : 0.9500\n",
            "[epoch:61, iteration:186000] train loss : 0.0115 train accuracy : 1.0000\n",
            "[epoch:61, iteration:186000] test_loss : 0.4971 test accuracy : 0.9035\n",
            "[epoch:62, iteration:186200] train loss : 0.0004 train accuracy : 1.0000\n",
            "[epoch:62, iteration:186400] train loss : 0.0047 train accuracy : 1.0000\n",
            "[epoch:62, iteration:186600] train loss : 0.0128 train accuracy : 1.0000\n",
            "[epoch:62, iteration:186800] train loss : 0.0933 train accuracy : 0.9500\n",
            "[epoch:62, iteration:187000] train loss : 0.0105 train accuracy : 1.0000\n",
            "[epoch:62, iteration:187200] train loss : 0.1147 train accuracy : 0.9500\n",
            "[epoch:62, iteration:187400] train loss : 0.0162 train accuracy : 1.0000\n",
            "[epoch:62, iteration:187600] train loss : 0.0537 train accuracy : 0.9500\n",
            "[epoch:62, iteration:187800] train loss : 0.0035 train accuracy : 1.0000\n",
            "[epoch:62, iteration:188000] train loss : 0.3166 train accuracy : 0.9000\n",
            "[epoch:62, iteration:188200] train loss : 0.0231 train accuracy : 1.0000\n",
            "[epoch:62, iteration:188400] train loss : 0.1439 train accuracy : 0.9500\n",
            "[epoch:62, iteration:188600] train loss : 0.0198 train accuracy : 1.0000\n",
            "[epoch:62, iteration:188800] train loss : 0.0464 train accuracy : 1.0000\n",
            "[epoch:62, iteration:189000] train loss : 0.0742 train accuracy : 0.9500\n",
            "[epoch:62, iteration:189000] test_loss : 0.5076 test accuracy : 0.9044\n",
            "[epoch:63, iteration:189200] train loss : 0.0046 train accuracy : 1.0000\n",
            "[epoch:63, iteration:189400] train loss : 0.0153 train accuracy : 1.0000\n",
            "[epoch:63, iteration:189600] train loss : 0.0282 train accuracy : 1.0000\n",
            "[epoch:63, iteration:189800] train loss : 0.0231 train accuracy : 1.0000\n",
            "[epoch:63, iteration:190000] train loss : 0.2118 train accuracy : 0.9000\n",
            "[epoch:63, iteration:190200] train loss : 0.0043 train accuracy : 1.0000\n",
            "[epoch:63, iteration:190400] train loss : 0.0853 train accuracy : 0.9500\n",
            "[epoch:63, iteration:190600] train loss : 0.0126 train accuracy : 1.0000\n",
            "[epoch:63, iteration:190800] train loss : 0.0485 train accuracy : 0.9500\n",
            "[epoch:63, iteration:191000] train loss : 0.0303 train accuracy : 1.0000\n",
            "[epoch:63, iteration:191200] train loss : 0.2285 train accuracy : 0.9500\n",
            "[epoch:63, iteration:191400] train loss : 0.0037 train accuracy : 1.0000\n",
            "[epoch:63, iteration:191600] train loss : 0.0239 train accuracy : 1.0000\n",
            "[epoch:63, iteration:191800] train loss : 0.0243 train accuracy : 1.0000\n",
            "[epoch:63, iteration:192000] train loss : 0.0130 train accuracy : 1.0000\n",
            "[epoch:63, iteration:192000] test_loss : 0.5059 test accuracy : 0.9040\n",
            "[epoch:64, iteration:192200] train loss : 0.0108 train accuracy : 1.0000\n",
            "[epoch:64, iteration:192400] train loss : 0.0031 train accuracy : 1.0000\n",
            "[epoch:64, iteration:192600] train loss : 0.2271 train accuracy : 0.9000\n",
            "[epoch:64, iteration:192800] train loss : 0.0020 train accuracy : 1.0000\n",
            "[epoch:64, iteration:193000] train loss : 0.0210 train accuracy : 1.0000\n",
            "[epoch:64, iteration:193200] train loss : 0.0279 train accuracy : 1.0000\n",
            "[epoch:64, iteration:193400] train loss : 0.0258 train accuracy : 1.0000\n",
            "[epoch:64, iteration:193600] train loss : 0.0048 train accuracy : 1.0000\n",
            "[epoch:64, iteration:193800] train loss : 0.0042 train accuracy : 1.0000\n",
            "[epoch:64, iteration:194000] train loss : 0.1323 train accuracy : 0.9500\n",
            "[epoch:64, iteration:194200] train loss : 0.0166 train accuracy : 1.0000\n",
            "[epoch:64, iteration:194400] train loss : 0.0084 train accuracy : 1.0000\n",
            "[epoch:64, iteration:194600] train loss : 0.1380 train accuracy : 0.9500\n",
            "[epoch:64, iteration:194800] train loss : 0.0190 train accuracy : 1.0000\n",
            "[epoch:64, iteration:195000] train loss : 0.0124 train accuracy : 1.0000\n",
            "[epoch:64, iteration:195000] test_loss : 0.5164 test accuracy : 0.9032\n",
            "[epoch:65, iteration:195200] train loss : 0.0394 train accuracy : 1.0000\n",
            "[epoch:65, iteration:195400] train loss : 0.0890 train accuracy : 0.9500\n",
            "[epoch:65, iteration:195600] train loss : 0.0051 train accuracy : 1.0000\n",
            "[epoch:65, iteration:195800] train loss : 0.0403 train accuracy : 1.0000\n",
            "[epoch:65, iteration:196000] train loss : 0.0150 train accuracy : 1.0000\n",
            "[epoch:65, iteration:196200] train loss : 0.0179 train accuracy : 1.0000\n",
            "[epoch:65, iteration:196400] train loss : 0.0130 train accuracy : 1.0000\n",
            "[epoch:65, iteration:196600] train loss : 0.0199 train accuracy : 1.0000\n",
            "[epoch:65, iteration:196800] train loss : 0.0009 train accuracy : 1.0000\n",
            "[epoch:65, iteration:197000] train loss : 0.1354 train accuracy : 0.9500\n",
            "[epoch:65, iteration:197200] train loss : 0.0238 train accuracy : 1.0000\n",
            "[epoch:65, iteration:197400] train loss : 0.2326 train accuracy : 0.9500\n",
            "[epoch:65, iteration:197600] train loss : 0.0133 train accuracy : 1.0000\n",
            "[epoch:65, iteration:197800] train loss : 0.0083 train accuracy : 1.0000\n",
            "[epoch:65, iteration:198000] train loss : 0.0024 train accuracy : 1.0000\n",
            "[epoch:65, iteration:198000] test_loss : 0.5323 test accuracy : 0.9028\n",
            "[epoch:66, iteration:198200] train loss : 0.0030 train accuracy : 1.0000\n",
            "[epoch:66, iteration:198400] train loss : 0.0014 train accuracy : 1.0000\n",
            "[epoch:66, iteration:198600] train loss : 0.0738 train accuracy : 0.9500\n",
            "[epoch:66, iteration:198800] train loss : 0.0063 train accuracy : 1.0000\n",
            "[epoch:66, iteration:199000] train loss : 0.1198 train accuracy : 0.9500\n",
            "[epoch:66, iteration:199200] train loss : 0.0910 train accuracy : 0.9500\n",
            "[epoch:66, iteration:199400] train loss : 0.0008 train accuracy : 1.0000\n",
            "[epoch:66, iteration:199600] train loss : 0.0978 train accuracy : 0.9500\n",
            "[epoch:66, iteration:199800] train loss : 0.0070 train accuracy : 1.0000\n",
            "[epoch:66, iteration:200000] train loss : 0.0533 train accuracy : 1.0000\n",
            "[epoch:66, iteration:200200] train loss : 0.0021 train accuracy : 1.0000\n",
            "[epoch:66, iteration:200400] train loss : 0.0209 train accuracy : 1.0000\n",
            "[epoch:66, iteration:200600] train loss : 0.0389 train accuracy : 0.9500\n",
            "[epoch:66, iteration:200800] train loss : 0.0674 train accuracy : 0.9500\n",
            "[epoch:66, iteration:201000] train loss : 0.0053 train accuracy : 1.0000\n",
            "[epoch:66, iteration:201000] test_loss : 0.5408 test accuracy : 0.9046\n",
            "[epoch:67, iteration:201200] train loss : 0.0057 train accuracy : 1.0000\n",
            "[epoch:67, iteration:201400] train loss : 0.0293 train accuracy : 1.0000\n",
            "[epoch:67, iteration:201600] train loss : 0.0365 train accuracy : 1.0000\n",
            "[epoch:67, iteration:201800] train loss : 0.0050 train accuracy : 1.0000\n",
            "[epoch:67, iteration:202000] train loss : 0.0412 train accuracy : 1.0000\n",
            "[epoch:67, iteration:202200] train loss : 0.0215 train accuracy : 1.0000\n",
            "[epoch:67, iteration:202400] train loss : 0.0025 train accuracy : 1.0000\n",
            "[epoch:67, iteration:202600] train loss : 0.0640 train accuracy : 0.9500\n",
            "[epoch:67, iteration:202800] train loss : 0.0089 train accuracy : 1.0000\n",
            "[epoch:67, iteration:203000] train loss : 0.0020 train accuracy : 1.0000\n",
            "[epoch:67, iteration:203200] train loss : 0.0158 train accuracy : 1.0000\n",
            "[epoch:67, iteration:203400] train loss : 0.0050 train accuracy : 1.0000\n",
            "[epoch:67, iteration:203600] train loss : 0.0113 train accuracy : 1.0000\n",
            "[epoch:67, iteration:203800] train loss : 0.0057 train accuracy : 1.0000\n",
            "[epoch:67, iteration:204000] train loss : 0.0137 train accuracy : 1.0000\n",
            "[epoch:67, iteration:204000] test_loss : 0.5634 test accuracy : 0.9024\n",
            "[epoch:68, iteration:204200] train loss : 0.0229 train accuracy : 1.0000\n",
            "[epoch:68, iteration:204400] train loss : 0.0088 train accuracy : 1.0000\n",
            "[epoch:68, iteration:204600] train loss : 0.1596 train accuracy : 0.9500\n",
            "[epoch:68, iteration:204800] train loss : 0.0524 train accuracy : 0.9500\n",
            "[epoch:68, iteration:205000] train loss : 0.0007 train accuracy : 1.0000\n",
            "[epoch:68, iteration:205200] train loss : 0.0036 train accuracy : 1.0000\n",
            "[epoch:68, iteration:205400] train loss : 0.0060 train accuracy : 1.0000\n",
            "[epoch:68, iteration:205600] train loss : 0.0001 train accuracy : 1.0000\n",
            "[epoch:68, iteration:205800] train loss : 0.0096 train accuracy : 1.0000\n",
            "[epoch:68, iteration:206000] train loss : 0.0115 train accuracy : 1.0000\n",
            "[epoch:68, iteration:206200] train loss : 0.0045 train accuracy : 1.0000\n",
            "[epoch:68, iteration:206400] train loss : 0.0011 train accuracy : 1.0000\n",
            "[epoch:68, iteration:206600] train loss : 0.2597 train accuracy : 0.9000\n",
            "[epoch:68, iteration:206800] train loss : 0.0834 train accuracy : 0.9500\n",
            "[epoch:68, iteration:207000] train loss : 0.1241 train accuracy : 0.9000\n",
            "[epoch:68, iteration:207000] test_loss : 0.5592 test accuracy : 0.9032\n",
            "[epoch:69, iteration:207200] train loss : 0.0003 train accuracy : 1.0000\n",
            "[epoch:69, iteration:207400] train loss : 0.0117 train accuracy : 1.0000\n",
            "[epoch:69, iteration:207600] train loss : 0.0015 train accuracy : 1.0000\n",
            "[epoch:69, iteration:207800] train loss : 0.0034 train accuracy : 1.0000\n",
            "[epoch:69, iteration:208000] train loss : 0.2260 train accuracy : 0.9000\n",
            "[epoch:69, iteration:208200] train loss : 0.0037 train accuracy : 1.0000\n",
            "[epoch:69, iteration:208400] train loss : 0.0149 train accuracy : 1.0000\n",
            "[epoch:69, iteration:208600] train loss : 0.0350 train accuracy : 1.0000\n",
            "[epoch:69, iteration:208800] train loss : 0.0133 train accuracy : 1.0000\n",
            "[epoch:69, iteration:209000] train loss : 0.0014 train accuracy : 1.0000\n",
            "[epoch:69, iteration:209200] train loss : 0.0079 train accuracy : 1.0000\n",
            "[epoch:69, iteration:209400] train loss : 0.0037 train accuracy : 1.0000\n",
            "[epoch:69, iteration:209600] train loss : 0.0128 train accuracy : 1.0000\n",
            "[epoch:69, iteration:209800] train loss : 0.0288 train accuracy : 1.0000\n",
            "[epoch:69, iteration:210000] train loss : 0.1673 train accuracy : 0.9000\n",
            "[epoch:69, iteration:210000] test_loss : 0.5686 test accuracy : 0.9030\n",
            "[epoch:70, iteration:210200] train loss : 0.0053 train accuracy : 1.0000\n",
            "[epoch:70, iteration:210400] train loss : 0.0095 train accuracy : 1.0000\n",
            "[epoch:70, iteration:210600] train loss : 0.1497 train accuracy : 0.9500\n",
            "[epoch:70, iteration:210800] train loss : 0.0379 train accuracy : 1.0000\n",
            "[epoch:70, iteration:211000] train loss : 0.1291 train accuracy : 0.9500\n",
            "[epoch:70, iteration:211200] train loss : 0.0034 train accuracy : 1.0000\n",
            "[epoch:70, iteration:211400] train loss : 0.1256 train accuracy : 0.9500\n",
            "[epoch:70, iteration:211600] train loss : 0.0163 train accuracy : 1.0000\n",
            "[epoch:70, iteration:211800] train loss : 0.0042 train accuracy : 1.0000\n",
            "[epoch:70, iteration:212000] train loss : 0.0028 train accuracy : 1.0000\n",
            "[epoch:70, iteration:212200] train loss : 0.1077 train accuracy : 0.9500\n",
            "[epoch:70, iteration:212400] train loss : 0.0044 train accuracy : 1.0000\n",
            "[epoch:70, iteration:212600] train loss : 0.0594 train accuracy : 0.9500\n",
            "[epoch:70, iteration:212800] train loss : 0.1326 train accuracy : 0.9500\n",
            "[epoch:70, iteration:213000] train loss : 0.0187 train accuracy : 1.0000\n",
            "[epoch:70, iteration:213000] test_loss : 0.6047 test accuracy : 0.9003\n",
            "[epoch:71, iteration:213200] train loss : 0.0068 train accuracy : 1.0000\n",
            "[epoch:71, iteration:213400] train loss : 0.0284 train accuracy : 1.0000\n",
            "[epoch:71, iteration:213600] train loss : 0.0384 train accuracy : 1.0000\n",
            "[epoch:71, iteration:213800] train loss : 0.0456 train accuracy : 0.9500\n",
            "[epoch:71, iteration:214000] train loss : 0.0091 train accuracy : 1.0000\n",
            "[epoch:71, iteration:214200] train loss : 0.0670 train accuracy : 0.9500\n",
            "[epoch:71, iteration:214400] train loss : 0.0012 train accuracy : 1.0000\n",
            "[epoch:71, iteration:214600] train loss : 0.0651 train accuracy : 0.9500\n",
            "[epoch:71, iteration:214800] train loss : 0.0465 train accuracy : 1.0000\n",
            "[epoch:71, iteration:215000] train loss : 0.0358 train accuracy : 1.0000\n",
            "[epoch:71, iteration:215200] train loss : 0.0768 train accuracy : 0.9500\n",
            "[epoch:71, iteration:215400] train loss : 0.1472 train accuracy : 0.9500\n",
            "[epoch:71, iteration:215600] train loss : 0.0287 train accuracy : 1.0000\n",
            "[epoch:71, iteration:215800] train loss : 0.0207 train accuracy : 1.0000\n",
            "[epoch:71, iteration:216000] train loss : 0.0138 train accuracy : 1.0000\n",
            "[epoch:71, iteration:216000] test_loss : 0.5984 test accuracy : 0.9013\n",
            "[epoch:72, iteration:216200] train loss : 0.0053 train accuracy : 1.0000\n",
            "[epoch:72, iteration:216400] train loss : 0.0061 train accuracy : 1.0000\n",
            "[epoch:72, iteration:216600] train loss : 0.0138 train accuracy : 1.0000\n",
            "[epoch:72, iteration:216800] train loss : 0.0121 train accuracy : 1.0000\n",
            "[epoch:72, iteration:217000] train loss : 0.0014 train accuracy : 1.0000\n",
            "[epoch:72, iteration:217200] train loss : 0.0209 train accuracy : 1.0000\n",
            "[epoch:72, iteration:217400] train loss : 0.0092 train accuracy : 1.0000\n",
            "[epoch:72, iteration:217600] train loss : 0.0366 train accuracy : 1.0000\n",
            "[epoch:72, iteration:217800] train loss : 0.0012 train accuracy : 1.0000\n",
            "[epoch:72, iteration:218000] train loss : 0.0326 train accuracy : 1.0000\n",
            "[epoch:72, iteration:218200] train loss : 0.0050 train accuracy : 1.0000\n",
            "[epoch:72, iteration:218400] train loss : 0.0152 train accuracy : 1.0000\n",
            "[epoch:72, iteration:218600] train loss : 0.0220 train accuracy : 1.0000\n",
            "[epoch:72, iteration:218800] train loss : 0.0781 train accuracy : 0.9500\n",
            "[epoch:72, iteration:219000] train loss : 0.2488 train accuracy : 0.8500\n",
            "[epoch:72, iteration:219000] test_loss : 0.6081 test accuracy : 0.9021\n",
            "[epoch:73, iteration:219200] train loss : 0.0050 train accuracy : 1.0000\n",
            "[epoch:73, iteration:219400] train loss : 0.0071 train accuracy : 1.0000\n",
            "[epoch:73, iteration:219600] train loss : 0.0290 train accuracy : 1.0000\n",
            "[epoch:73, iteration:219800] train loss : 0.0035 train accuracy : 1.0000\n",
            "[epoch:73, iteration:220000] train loss : 0.0053 train accuracy : 1.0000\n",
            "[epoch:73, iteration:220200] train loss : 0.0281 train accuracy : 1.0000\n",
            "[epoch:73, iteration:220400] train loss : 0.0015 train accuracy : 1.0000\n",
            "[epoch:73, iteration:220600] train loss : 0.0012 train accuracy : 1.0000\n",
            "[epoch:73, iteration:220800] train loss : 0.0009 train accuracy : 1.0000\n",
            "[epoch:73, iteration:221000] train loss : 0.0124 train accuracy : 1.0000\n",
            "[epoch:73, iteration:221200] train loss : 0.0033 train accuracy : 1.0000\n",
            "[epoch:73, iteration:221400] train loss : 0.0696 train accuracy : 0.9500\n",
            "[epoch:73, iteration:221600] train loss : 0.0119 train accuracy : 1.0000\n",
            "[epoch:73, iteration:221800] train loss : 0.0179 train accuracy : 1.0000\n",
            "[epoch:73, iteration:222000] train loss : 0.0630 train accuracy : 1.0000\n",
            "[epoch:73, iteration:222000] test_loss : 0.6122 test accuracy : 0.9017\n",
            "[epoch:74, iteration:222200] train loss : 0.0096 train accuracy : 1.0000\n",
            "[epoch:74, iteration:222400] train loss : 0.0070 train accuracy : 1.0000\n",
            "[epoch:74, iteration:222600] train loss : 0.0873 train accuracy : 0.9500\n",
            "[epoch:74, iteration:222800] train loss : 0.0298 train accuracy : 1.0000\n",
            "[epoch:74, iteration:223000] train loss : 0.0057 train accuracy : 1.0000\n",
            "[epoch:74, iteration:223200] train loss : 0.0042 train accuracy : 1.0000\n",
            "[epoch:74, iteration:223400] train loss : 0.0196 train accuracy : 1.0000\n",
            "[epoch:74, iteration:223600] train loss : 0.0295 train accuracy : 1.0000\n",
            "[epoch:74, iteration:223800] train loss : 0.1245 train accuracy : 0.9500\n",
            "[epoch:74, iteration:224000] train loss : 0.0792 train accuracy : 0.9500\n",
            "[epoch:74, iteration:224200] train loss : 0.0125 train accuracy : 1.0000\n",
            "[epoch:74, iteration:224400] train loss : 0.0541 train accuracy : 0.9500\n",
            "[epoch:74, iteration:224600] train loss : 0.0171 train accuracy : 1.0000\n",
            "[epoch:74, iteration:224800] train loss : 0.0255 train accuracy : 1.0000\n",
            "[epoch:74, iteration:225000] train loss : 0.0044 train accuracy : 1.0000\n",
            "[epoch:74, iteration:225000] test_loss : 0.6277 test accuracy : 0.9016\n",
            "[epoch:75, iteration:225200] train loss : 0.0218 train accuracy : 1.0000\n",
            "[epoch:75, iteration:225400] train loss : 0.0036 train accuracy : 1.0000\n",
            "[epoch:75, iteration:225600] train loss : 0.0466 train accuracy : 0.9500\n",
            "[epoch:75, iteration:225800] train loss : 0.0323 train accuracy : 1.0000\n",
            "[epoch:75, iteration:226000] train loss : 0.0000 train accuracy : 1.0000\n",
            "[epoch:75, iteration:226200] train loss : 0.2905 train accuracy : 0.9500\n",
            "[epoch:75, iteration:226400] train loss : 0.0313 train accuracy : 1.0000\n",
            "[epoch:75, iteration:226600] train loss : 0.0062 train accuracy : 1.0000\n",
            "[epoch:75, iteration:226800] train loss : 0.0011 train accuracy : 1.0000\n",
            "[epoch:75, iteration:227000] train loss : 0.0162 train accuracy : 1.0000\n",
            "[epoch:75, iteration:227200] train loss : 0.0523 train accuracy : 0.9500\n",
            "[epoch:75, iteration:227400] train loss : 0.0672 train accuracy : 0.9500\n",
            "[epoch:75, iteration:227600] train loss : 0.0035 train accuracy : 1.0000\n",
            "[epoch:75, iteration:227800] train loss : 0.0010 train accuracy : 1.0000\n",
            "[epoch:75, iteration:228000] train loss : 0.0093 train accuracy : 1.0000\n",
            "[epoch:75, iteration:228000] test_loss : 0.6378 test accuracy : 0.8988\n",
            "[epoch:76, iteration:228200] train loss : 0.0003 train accuracy : 1.0000\n",
            "[epoch:76, iteration:228400] train loss : 0.0015 train accuracy : 1.0000\n",
            "[epoch:76, iteration:228600] train loss : 0.0091 train accuracy : 1.0000\n",
            "[epoch:76, iteration:228800] train loss : 0.0276 train accuracy : 1.0000\n",
            "[epoch:76, iteration:229000] train loss : 0.0051 train accuracy : 1.0000\n",
            "[epoch:76, iteration:229200] train loss : 0.0011 train accuracy : 1.0000\n",
            "[epoch:76, iteration:229400] train loss : 0.0138 train accuracy : 1.0000\n",
            "[epoch:76, iteration:229600] train loss : 0.0744 train accuracy : 0.9500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-40e1dbce4abd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# backprogate loss.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# update the weights in the network.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECu3yS0OvfoR",
        "colab_type": "text"
      },
      "source": [
        "## Step 9: Visualize and analyze the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G89sqVp-vLRy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "d3e358a5-2512-48a9-d9ff-6be08ef7194b"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(train_losses, label='train loss')\n",
        "plt.plot(test_losses, label='test loss')\n",
        "plt.legend()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f6482ba5160>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHjD9SGrvhzc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "outputId": "f6c5d6db-d30a-4614-9501-5f8197f7da05"
      },
      "source": [
        "import random\n",
        "from PIL import Image\n",
        "\n",
        "my_classifier.eval()\n",
        "\n",
        "num_test_samples = len(test_dataset)\n",
        "random_idx = random.randint(0, num_test_samples)\n",
        "\n",
        "topil = transforms.transforms.ToPILImage()\n",
        "test_input, test_label = test_dataset.__getitem__(random_idx)\n",
        "test_prediction = F.softmax(my_classifier(test_input.unsqueeze(0).to(device)), dim=1).argmax().item()\n",
        "print('label : %i' % test_label)\n",
        "print('prediction : %i' % test_prediction)\n",
        "\n",
        "test_image = topil(test_input)\n",
        "test_image.resize((128, 128))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "label : 4\n",
            "prediction : 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAAAAADmVT4XAAAEdElEQVR4nO3bOY9cRRAH8N8ce9iz\n9hrZFgiDBQ64Ao4IGRGAREJKxhcgQQSkRAR8Cr4AKYgAiZAQESCLSwZZAgJkkLHlAx+7M0PQ1b3v\nWuQEtYPX0jz1667u91f1X1Xd1TUT91OexDvwKX6CU3gMNmCGfXgFn8O39zUvpvcr+H+VEUB1AJP/\naJ/CEr6Q+OUKTuf+SUv6MuzgFzg//KV1r7m6BkYA8/brVKzTWiw/OIHrouk3kunZgqskazQXTVdb\nM5al7y9/+WLVMgKoDqBDwhZbnoEPYVOQcEe4vVW0pgmO5eZrOAtv4xPN2UZD9KACOMwZvYv3YQ/u\nSKbHhsC8Fhy4BUeFWbqRRxzPXR8RbBgu1TUwAqgOYICEJ+EriVEJ4Y5kehDGZAPbslBuMZcYaw9H\nZKHz+GMYQHUNjADm/aa3SIbkH2Jp14IDk3ixxD1ixRf57W7I2Gp2+QDvDQOoroERQHUAAyR8lUSp\nXcLQlIPYSndXMyfRdUM8ViSSTuE2nDscQHUNjACqAxgg4ctwU9psH2zAit8smCflMctNexomc4sw\nkGfysOJSS6mugRHAAAdOwa9iwabEsYuGNyyVe6TVLk5yLsZulP4FnoBLva9V18AIoDqADgm34ncQ\noJqR7MuyJV5M04yGkZpFl00N/2kbb8DHPQDVNTAC6HBgN1fW4lS1JO22Z7m5HeyZNEeYhHByZDT8\n1HPDAKprYARQHUCHhGdypZz9ZiReLUhMKxcpExo7prIRKvZnnlss8fwwgOoaGAF0OHBaw8YsCWu0\n0o0R9aPOhRFr0h7qIP60j0eHAVTXwAigOoAOCU8KEt0RJDyWpdrxgXWzMrAjWsZL+sBSwz+2S3UN\njACqAzjs7rhEm6Ykv7hFY0tW3F7xhu1D5L640pnkt5GEDyqADge2dU3LJo3z/0zYp5Um+ElvxF5u\nmZM4MCURodyx4AHQwAigOoAOCR/JlXXu+4Z0ZFyQYt/7NGhXHntC6C7JdM0IypYN2rP4uvXF6hoY\nAXQ4cEKsWvFKP8Cb4hLvjjA0JYZUogWz3L8Q0sfg7yy0hod7AKprYARQHUCHhDsi92RL3LhdI3Hz\nMolpbdqVt7J12s7N38FTYqZ1nqhdqmtgBDDvv05Ju+J9+HlYfDJcOSIs1i6+hJfgzyz0UA9AdQ2M\nAKoD6JCwddC/SyN/svS3aVceJVB5Gx7H90S0c577z/YAVNfACKA6gAESLnNlTcO3DYQFWsNaN3pm\nuEiQeENQeqcHoLoGRgAD13a3aMR4rtAIDbXCAqvyyDKmYqO00iTLVKSGL3oAqmtgBFAdQIeEVzWS\nUCbwYu4q2QLtbXmhXD/j6QUit/d6c1i7VNfACKDDgYt4ncbf1y4Qh3ySfVqKxyaRu1sucucaySaX\nSBlRyaMxJrONAIZKh4Q3BYk24+dHUibc71n8OI20XTTsz408fiH4eYJGbt1fPQDVNTACOOyPTkfF\nQR+8hqdJS3su9x9MUC7LLohgwGeCA0geaaI1ZS7VNTACqA7gX3qt8wlhk4ncAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=128x128 at 0x7F6480220898>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    }
  ]
}